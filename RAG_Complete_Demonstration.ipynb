{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìö Complete RAG Application Demonstration\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates a **production-grade RAG (Retrieval-Augmented Generation) system** from scratch.\n",
    "\n",
    "### What is RAG?\n",
    "RAG combines:\n",
    "- **Retrieval**: Finding relevant information from documents\n",
    "- **Generation**: Using LLMs to generate answers based on retrieved context\n",
    "\n",
    "### Key Components:\n",
    "1. üîß **Docling** - Advanced PDF processing with table extraction\n",
    "2. ü§ñ **Ollama** - Local LLM hosting (llama3.1, mistral)\n",
    "3. üßÆ **Nomic-AI** - High-quality embeddings from Hugging Face\n",
    "4. üìä **FAISS** - Fast vector similarity search\n",
    "5. üéØ **BGE Reranker** - Improve retrieval precision\n",
    "6. üìà **RAGAS** - Automated evaluation framework\n",
    "\n",
    "### What Makes This Production-Grade?\n",
    "‚úÖ Handles **mixed data**: PDFs, CSVs, tables, images\n",
    "‚úÖ **Smart routing**: Automatically determines if query needs table or text data\n",
    "‚úÖ **Metadata enrichment**: Source citation and filtering\n",
    "‚úÖ **Reranking**: Two-stage retrieval for better accuracy\n",
    "‚úÖ **Evaluation metrics**: Objective quality measurement\n",
    "‚úÖ **Error handling**: Robust and reliable\n",
    "\n",
    "---\n",
    "\n",
    "## üìñ Table of Contents\n",
    "1. [Installation](#setup)\n",
    "2. [Environment Setup](#environment)\n",
    "3. [Data Ingestion](#ingestion) - Docling PDF + table extraction\n",
    "4. [Text Chunking](#chunking) - Recursive splitting\n",
    "5. [Embeddings](#embeddings) - Nomic-AI vectors\n",
    "6. [Vector Store](#vectorstore) - FAISS setup\n",
    "7. [Retrieval](#retrieval) - Semantic search\n",
    "8. [Reranking](#reranking) - Cross-encoder reranking\n",
    "9. [RAG Pipeline](#pipeline) - Complete system\n",
    "10. [Evaluation](#evaluation) - RAGAS metrics\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Let's Begin!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìñ How to Use This Notebook\n",
    "\n",
    "### Quick Start:\n",
    "1. **Run cells sequentially** from top to bottom\n",
    "2. **Don't skip cells** - each builds on previous ones\n",
    "3. **Wait for Ollama** - Ensure `ollama serve` is running\n",
    "4. **Check outputs** - Verify each step before proceeding\n",
    "\n",
    "### Cell Types:\n",
    "- üü¶ **Blue markdown** - Explanations and context\n",
    "- ‚¨ú **Code cells** - Run these to execute the pipeline\n",
    "\n",
    "### Estimated Runtime:\n",
    "- First run: ~15-20 minutes (downloads models)\n",
    "- Subsequent runs: ~5-10 minutes\n",
    "\n",
    "### Troubleshooting:\n",
    "- **Ollama errors**: Check if `ollama serve` is running\n",
    "- **Import errors**: Re-run installation cell\n",
    "- **Memory errors**: Reduce `chunk_size` or `retrieval_k`\n",
    "- **Slow performance**: Use smaller models (phi3 instead of llama3.1)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. üîß Setup and Installation\n",
    "\n",
    "### Prerequisites:\n",
    "- Python 3.9+\n",
    "- Ollama installed and running (`brew install ollama` or download from ollama.ai)\n",
    "- 8GB+ RAM recommended\n",
    "- ~5GB disk space for models\n",
    "\n",
    "### What we'll install:\n",
    "| Package | Purpose |\n",
    "|---------|---------|\n",
    "| `langchain` | RAG framework |\n",
    "| `faiss-cpu` | Vector database |\n",
    "| `docling` | PDF + table extraction |\n",
    "| `nomic` | Embedding models |\n",
    "| `sentence-transformers` | Reranking |\n",
    "| `ragas` | Evaluation metrics |\n",
    "| `pandas` | Data manipulation |\n",
    "\n",
    "**Estimated install time**: 2-3 minutes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/opt/homebrew/opt/python@3.11/bin/python3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/opt/homebrew/opt/python@3.11/bin/python3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/opt/homebrew/opt/python@3.11/bin/python3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/opt/homebrew/opt/python@3.11/bin/python3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/opt/homebrew/opt/python@3.11/bin/python3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/opt/homebrew/opt/python@3.11/bin/python3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/opt/homebrew/opt/python@3.11/bin/python3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/opt/homebrew/opt/python@3.11/bin/python3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/opt/homebrew/opt/python@3.11/bin/python3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/opt/homebrew/opt/python@3.11/bin/python3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/opt/homebrew/opt/python@3.11/bin/python3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "%pip install -q langchain langchain-community langchain-ollama\n",
    "%pip install -q faiss-cpu\n",
    "%pip install -q docling  # For advanced PDF processing with table extraction\n",
    "%pip install -q nomic einops\n",
    "%pip install -q cohere\n",
    "%pip install -q sentence-transformers\n",
    "%pip install -q ragas\n",
    "%pip install -q langsmith\n",
    "%pip install -q datasets\n",
    "%pip install -q python-dotenv\n",
    "%pip install -q pandas openpyxl  # For CSV/Excel processing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Initialize Ollama LLM\n",
    "\n",
    "**Ollama** allows you to run LLMs locally without API costs.\n",
    "\n",
    "**Popular models:**\n",
    "- `llama3.1` - Fast and capable (recommended)\n",
    "- `mistral` - Good balance of speed/quality\n",
    "- `phi3` - Lightweight option\n",
    "\n",
    "Make sure Ollama is running: `ollama serve`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì All imports successful!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# LangChain imports\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Other imports\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"‚úì All imports successful!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Environment configured!\n"
     ]
    }
   ],
   "source": [
    "# Environment setup\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Set API keys (you'll need to set these in your environment or .env file)\n",
    "# os.environ[\"COHERE_API_KEY\"] = \"your-cohere-api-key\"\n",
    "# os.environ[\"LANGCHAIN_API_KEY\"] = \"your-langsmith-api-key\"\n",
    "# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "# os.environ[\"LANGCHAIN_PROJECT\"] = \"rag-demonstration\"\n",
    "\n",
    "print(\"‚úì Environment configured!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Ingestion with Docling <a id='ingestion'></a>\n",
    "\n",
    "Docling is a powerful document processing library that can handle various document formats (PDF, DOCX, HTML, etc.) and extract structured content.\n",
    "\n",
    "### üéØ What We'll Demonstrate:\n",
    "\n",
    "1. **PDF Processing with Docling**\n",
    "   - Extract text content for RAG chunking and embeddings\n",
    "   - Extract tables and save to SQLite database\n",
    "   - Handle images and complex layouts\n",
    "   - Support OCR for scanned documents\n",
    "\n",
    "2. **SQL Table Storage**\n",
    "   - Store extracted tables in SQLite database\n",
    "   - Enable SQL querying on structured data\n",
    "   - Preserve table relationships and metadata\n",
    "\n",
    "3. **CSV/Excel File Processing**\n",
    "   - Load and parse CSV/Excel files with pandas\n",
    "   - Demonstrate data filtering and aggregation\n",
    "   - Show statistical analysis capabilities\n",
    "   - Prepare for natural language querying with RAG\n",
    "\n",
    "### üìã Use Cases:\n",
    "\n",
    "- **Hybrid RAG**: Combine unstructured text (PDFs) with structured data (tables, CSV)\n",
    "- **Document Analysis**: Extract and analyze tables from reports\n",
    "- **Data Integration**: Process multiple data sources in one pipeline\n",
    "- **SQL + Vector Search**: Query both relational data and semantic content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Documents in sample_documents folder:\n",
      "============================================================\n",
      "\n",
      "üìÑ PDF Files (3):\n",
      "  ‚Ä¢ placement_report_2023_24.pdf\n",
      "  ‚Ä¢ IBEnglish_2025.pdf\n",
      "  ‚Ä¢ brochure.pdf\n",
      "\n",
      "üìä CSV/Excel Files (4):\n",
      "  ‚Ä¢ IIT_Bombay_Course_Enrollment.csv\n",
      "  ‚Ä¢ IIT_Bombay_Research_Publications.csv\n",
      "  ‚Ä¢ IIT_Bombay_Admissions_2020_2023.csv\n",
      "  ‚Ä¢ IIT_Bombay_Placement_Stats.csv\n",
      "\n",
      "============================================================\n",
      "‚úì Document inventory complete!\n"
     ]
    }
   ],
   "source": [
    "# Setup: Check what documents we have in the sample_documents folder\n",
    "sample_docs_dir = Path(\"sample_documents\")\n",
    "sample_docs_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"üìÅ Documents in sample_documents folder:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "pdf_files = list(sample_docs_dir.glob(\"*.pdf\"))\n",
    "csv_files = list(sample_docs_dir.glob(\"*.csv\"))\n",
    "excel_files = list(sample_docs_dir.glob(\"*.xlsx\"))\n",
    "\n",
    "print(f\"\\nüìÑ PDF Files ({len(pdf_files)}):\")\n",
    "for pdf in pdf_files:\n",
    "    print(f\"  ‚Ä¢ {pdf.name}\")\n",
    "\n",
    "print(f\"\\nüìä CSV/Excel Files ({len(csv_files) + len(excel_files)}):\")\n",
    "for csv in csv_files:\n",
    "    print(f\"  ‚Ä¢ {csv.name}\")\n",
    "for xlsx in excel_files:\n",
    "    print(f\"  ‚Ä¢ {xlsx.name}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úì Document inventory complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 PDF Processing with Table Extraction\n",
    "\n",
    "**Goal**: Extract both text and tables from PDFs\n",
    "- **Text** ‚Üí Will be chunked and embedded for RAG\n",
    "- **Tables** ‚Üí Stored in SQLite database for structured queries\n",
    "\n",
    "**Why SQLite?** Tables have structure that benefits from SQL querying.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Initializing Docling PDF Processing...\n",
      "============================================================\n",
      "‚úì Docling DocumentConverter initialized\n",
      "  Tables will be automatically detected and extracted\n",
      "\n",
      "üìÑ Processing PDF Files with Docling...\n",
      "------------------------------------------------------------\n",
      "\n",
      "  Processing: placement_report_2023_24.pdf\n",
      "    üîÑ Converting with Docling (this may take a moment)...\n",
      "    ‚úì Docling conversion complete\n",
      "    üìä Found 21 tables in document\n",
      "    ‚úì Saved table 'placement_report_2023_24_table_1' (17 rows x 2 columns)\n",
      "    ‚úì Saved table 'placement_report_2023_24_table_2' (11 rows x 2 columns)\n",
      "    ‚úì Saved table 'placement_report_2023_24_table_3' (4 rows x 2 columns)\n",
      "    ‚úì Saved table 'placement_report_2023_24_table_4' (14 rows x 5 columns)\n",
      "    ‚úì Saved table 'placement_report_2023_24_table_5' (7 rows x 2 columns)\n",
      "    ‚úì Saved table 'placement_report_2023_24_table_6' (30 rows x 4 columns)\n",
      "    ‚úì Saved table 'placement_report_2023_24_table_7' (15 rows x 3 columns)\n",
      "    ‚úì Saved table 'placement_report_2023_24_table_8' (9 rows x 3 columns)\n",
      "    ‚úì Saved table 'placement_report_2023_24_table_9' (4 rows x 2 columns)\n",
      "    ‚úì Saved table 'placement_report_2023_24_table_10' (2 rows x 5 columns)\n",
      "    ‚úì Saved table 'placement_report_2023_24_table_11' (4 rows x 3 columns)\n",
      "    ‚úì Saved table 'placement_report_2023_24_table_12' (6 rows x 5 columns)\n",
      "    ‚úì Saved table 'placement_report_2023_24_table_13' (17 rows x 6 columns)\n",
      "    ‚úì Saved table 'placement_report_2023_24_table_14' (12 rows x 5 columns)\n",
      "    ‚úì Saved table 'placement_report_2023_24_table_15' (2 rows x 3 columns)\n",
      "    ‚úì Saved table 'placement_report_2023_24_table_16' (2 rows x 3 columns)\n",
      "    ‚úì Saved table 'placement_report_2023_24_table_17' (27 rows x 5 columns)\n",
      "    ‚úì Saved table 'placement_report_2023_24_table_18' (28 rows x 5 columns)\n",
      "    ‚úì Saved table 'placement_report_2023_24_table_19' (28 rows x 5 columns)\n",
      "    ‚úì Saved table 'placement_report_2023_24_table_20' (27 rows x 5 columns)\n",
      "    ‚úì Saved table 'placement_report_2023_24_table_21' (8 rows x 5 columns)\n",
      "    ‚úì Extracted text: 43919 characters\n",
      "\n",
      "  Processing: IBEnglish_2025.pdf\n",
      "    üîÑ Converting with Docling (this may take a moment)...\n",
      "    ‚úì Docling conversion complete\n",
      "    üìä Found 33 tables in document\n",
      "    ‚úì Saved table 'IBEnglish_2025_table_1' (35 rows x 2 columns)\n",
      "    ‚úì Saved table 'IBEnglish_2025_table_2' (33 rows x 2 columns)\n",
      "    ‚úì Saved table 'IBEnglish_2025_table_3' (23 rows x 4 columns)\n",
      "    ‚úì Saved table 'IBEnglish_2025_table_4' (9 rows x 3 columns)\n",
      "    ‚úì Saved table 'IBEnglish_2025_table_5' (2 rows x 2 columns)\n",
      "    ‚ö† Error extracting table 6: duplicate column name: Number_of_'Top'_candidates\n",
      "       Table attributes: ['self_ref', 'parent', 'children', 'content_layer', 'label', 'prov', 'captions', 'references', 'footnotes', 'image', 'data', 'annotations']\n",
      "    ‚úì Saved table 'IBEnglish_2025_table_7' (4 rows x 2 columns)\n",
      "    ‚úì Saved table 'IBEnglish_2025_table_8' (8 rows x 3 columns)\n",
      "    ‚úì Saved table 'IBEnglish_2025_table_9' (3 rows x 3 columns)\n",
      "    ‚úì Saved table 'IBEnglish_2025_table_10' (18 rows x 2 columns)\n",
      "    ‚úì Saved table 'IBEnglish_2025_table_11' (18 rows x 2 columns)\n",
      "    ‚úì Saved table 'IBEnglish_2025_table_12' (21 rows x 2 columns)\n",
      "    ‚úì Saved table 'IBEnglish_2025_table_13' (2 rows x 2 columns)\n",
      "    ‚úì Saved table 'IBEnglish_2025_table_14' (6 rows x 2 columns)\n",
      "    ‚úì Saved table 'IBEnglish_2025_table_15' (10 rows x 2 columns)\n",
      "    ‚úì Saved table 'IBEnglish_2025_table_16' (46 rows x 2 columns)\n",
      "    ‚úì Saved table 'IBEnglish_2025_table_17' (50 rows x 3 columns)\n",
      "    ‚úì Saved table 'IBEnglish_2025_table_18' (6 rows x 2 columns)\n",
      "    ‚úì Saved table 'IBEnglish_2025_table_19' (4 rows x 2 columns)\n",
      "    ‚úì Saved table 'IBEnglish_2025_table_20' (2 rows x 2 columns)\n",
      "    ‚úì Saved table 'IBEnglish_2025_table_21' (4 rows x 2 columns)\n",
      "    ‚úì Saved table 'IBEnglish_2025_table_22' (11 rows x 3 columns)\n",
      "    ‚úì Saved table 'IBEnglish_2025_table_23' (4 rows x 2 columns)\n",
      "    ‚úì Saved table 'IBEnglish_2025_table_24' (37 rows x 3 columns)\n",
      "    ‚úì Saved table 'IBEnglish_2025_table_25' (6 rows x 5 columns)\n",
      "    ‚úì Saved table 'IBEnglish_2025_table_26' (7 rows x 4 columns)\n",
      "    ‚úì Saved table 'IBEnglish_2025_table_27' (2 rows x 5 columns)\n",
      "    ‚úì Saved table 'IBEnglish_2025_table_28' (1 rows x 5 columns)\n",
      "    ‚úì Saved table 'IBEnglish_2025_table_29' (35 rows x 37 columns)\n",
      "    ‚úì Saved table 'IBEnglish_2025_table_30' (30 rows x 17 columns)\n",
      "    ‚úì Saved table 'IBEnglish_2025_table_31' (16 rows x 3 columns)\n",
      "    ‚úì Saved table 'IBEnglish_2025_table_32' (15 rows x 3 columns)\n",
      "    ‚úì Saved table 'IBEnglish_2025_table_33' (7 rows x 3 columns)\n",
      "    ‚úì Extracted text: 232656 characters\n",
      "\n",
      "  Processing: brochure.pdf\n",
      "    üîÑ Converting with Docling (this may take a moment)...\n",
      "    ‚úì Docling conversion complete\n",
      "    üìä Found 0 tables in document\n",
      "    ‚úì Extracted text: 17319 characters\n",
      "\n",
      "============================================================\n",
      "‚úì PDF Processing Complete!\n",
      "  ‚Ä¢ Documents processed: 3\n",
      "  ‚Ä¢ Tables extracted to SQL: 53\n",
      "  ‚Ä¢ Database: pdf_tables.db\n",
      "============================================================\n",
      "\n",
      "--- Sample Document ---\n",
      "Source: placement_report_2023_24.pdf\n",
      "Type: pdf\n",
      "Tables: 21\n",
      "Content preview:\n",
      "## Placement and Internship Report\n",
      "\n",
      "## Academic Year 2023 - 2024\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "Placement Office Indian Institute of Technology, Bombay\n",
      "\n",
      "## Index\n",
      "\n",
      "| Particulars                                                   |   Page No. |\n",
      "|---------------------------------------------------------------|-------...\n"
     ]
    }
   ],
   "source": [
    "# Advanced PDF Processing with Docling - Extract Tables to SQL and Text for RAG\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "from docling.document_converter import DocumentConverter\n",
    "\n",
    "# Initialize SQLite database for storing tables\n",
    "db_path = \"pdf_tables.db\"\n",
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "print(\"üîß Initializing Docling PDF Processing...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Initialize DocumentConverter - Docling handles table extraction automatically\n",
    "converter = DocumentConverter()\n",
    "print(\"‚úì Docling DocumentConverter initialized\")\n",
    "print(\"  Tables will be automatically detected and extracted\")\n",
    "\n",
    "def extract_tables_from_docling(result, pdf_name: str, conn) -> int:\n",
    "    \"\"\"\n",
    "    Extract tables from Docling result and save to SQL database.\n",
    "    \n",
    "    Args:\n",
    "        result: Docling conversion result\n",
    "        pdf_name: Name of the PDF file\n",
    "        conn: SQLite connection\n",
    "    \n",
    "    Returns:\n",
    "        Number of tables extracted\n",
    "    \"\"\"\n",
    "    table_count = 0\n",
    "    \n",
    "    try:\n",
    "        # Access tables from the Docling document\n",
    "        if hasattr(result, 'document') and hasattr(result.document, 'tables'):\n",
    "            tables = result.document.tables\n",
    "            print(f\"    üìä Found {len(tables)} tables in document\")\n",
    "            \n",
    "            for idx, table in enumerate(tables, 1):\n",
    "                try:\n",
    "                    # Docling tables have a data attribute that can be converted to DataFrame\n",
    "                    # Try different methods to extract table data\n",
    "                    \n",
    "                    # Method 1: Try to_dataframe if available\n",
    "                    if hasattr(table, 'to_dataframe'):\n",
    "                        df = table.to_dataframe()\n",
    "                    # Method 2: Try export_to_dataframe if available\n",
    "                    elif hasattr(table, 'export_to_dataframe'):\n",
    "                        df = table.export_to_dataframe()\n",
    "                    # Method 3: Access data directly\n",
    "                    elif hasattr(table, 'data'):\n",
    "                        # Convert table data to DataFrame\n",
    "                        table_data = table.data\n",
    "                        if isinstance(table_data, list) and len(table_data) > 0:\n",
    "                            df = pd.DataFrame(table_data[1:], columns=table_data[0])\n",
    "                        else:\n",
    "                            df = pd.DataFrame(table_data)\n",
    "                    else:\n",
    "                        print(f\"    ‚ö† Table {idx}: Unable to access table data\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Clean and save the DataFrame\n",
    "                    if df is not None and not df.empty:\n",
    "                        # Clean column names\n",
    "                        df.columns = [\n",
    "                            str(col).strip().replace(' ', '_').replace('-', '_').replace('.', '_')[:50] \n",
    "                            if str(col).strip() else f\"col_{i}\"\n",
    "                            for i, col in enumerate(df.columns)\n",
    "                        ]\n",
    "                        \n",
    "                        # Create table name\n",
    "                        table_name = f\"{pdf_name.replace('.pdf', '').replace(' ', '_').replace('-', '_')}_table_{idx}\"\n",
    "                        \n",
    "                        # Save to SQL\n",
    "                        df.to_sql(table_name, conn, if_exists='replace', index=False)\n",
    "                        table_count += 1\n",
    "                        print(f\"    ‚úì Saved table '{table_name}' ({len(df)} rows x {len(df.columns)} columns)\")\n",
    "                    else:\n",
    "                        print(f\"    ‚ö† Table {idx}: Empty or None\")\n",
    "                        \n",
    "                except Exception as table_err:\n",
    "                    print(f\"    ‚ö† Error extracting table {idx}: {str(table_err)}\")\n",
    "                    # Try to print table structure for debugging\n",
    "                    if hasattr(table, '__dict__'):\n",
    "                        print(f\"       Table attributes: {list(table.__dict__.keys())}\")\n",
    "                    continue\n",
    "        else:\n",
    "            print(f\"    ‚Ñπ No tables attribute found in document\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"    ‚ö† Table extraction error: {str(e)}\")\n",
    "    \n",
    "    return table_count\n",
    "\n",
    "def ingest_pdfs_with_docling(directory_path: Path, conn) -> tuple[List[Document], int]:\n",
    "    \"\"\"\n",
    "    Ingest PDFs using Docling: extract tables to SQL and text for RAG.\n",
    "    \n",
    "    Args:\n",
    "        directory_path: Path to directory containing PDFs\n",
    "        conn: SQLite connection for storing tables\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (List of text Documents for RAG, Total tables extracted)\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    total_tables = 0\n",
    "    \n",
    "    print(\"\\nüìÑ Processing PDF Files with Docling...\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    pdf_files = list(directory_path.glob(\"*.pdf\"))\n",
    "    \n",
    "    if not pdf_files:\n",
    "        print(\"‚ö† No PDF files found!\")\n",
    "        return documents, total_tables\n",
    "    \n",
    "    for pdf_path in pdf_files:\n",
    "        try:\n",
    "            print(f\"\\n  Processing: {pdf_path.name}\")\n",
    "            print(f\"    üîÑ Converting with Docling (this may take a moment)...\")\n",
    "            \n",
    "            # Convert PDF with Docling\n",
    "            result = converter.convert(pdf_path)\n",
    "            \n",
    "            print(f\"    ‚úì Docling conversion complete\")\n",
    "            \n",
    "            # Extract tables using Docling\n",
    "            table_count = extract_tables_from_docling(result, pdf_path.name, conn)\n",
    "            total_tables += table_count\n",
    "            \n",
    "            # Extract text for RAG\n",
    "            # Try markdown export first (preserves structure)\n",
    "            try:\n",
    "                if hasattr(result.document, 'export_to_markdown'):\n",
    "                    text_content = result.document.export_to_markdown()\n",
    "                elif hasattr(result.document, 'export_to_text'):\n",
    "                    text_content = result.document.export_to_text()\n",
    "                else:\n",
    "                    # Fallback: get text from document\n",
    "                    text_content = str(result.document)\n",
    "                \n",
    "                print(f\"    ‚úì Extracted text: {len(text_content)} characters\")\n",
    "                \n",
    "            except Exception as text_err:\n",
    "                print(f\"    ‚ö† Text extraction error: {str(text_err)}\")\n",
    "                text_content = \"Error extracting text\"\n",
    "            \n",
    "            # Create document for RAG\n",
    "            doc = Document(\n",
    "                page_content=text_content,\n",
    "                metadata={\n",
    "                    \"source\": str(pdf_path),\n",
    "                    \"filename\": pdf_path.name,\n",
    "                    \"type\": \"pdf\",\n",
    "                    \"tables_extracted\": table_count,\n",
    "                    \"ingestion_timestamp\": datetime.now().isoformat(),\n",
    "                    \"extraction_method\": \"docling\"\n",
    "                }\n",
    "            )\n",
    "            documents.append(doc)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    ‚úó Error processing {pdf_path.name}: {str(e)}\")\n",
    "            print(f\"    ‚Ñπ Error details: {type(e).__name__}\")\n",
    "            import traceback\n",
    "            print(f\"    ‚Ñπ Traceback: {traceback.format_exc()[:200]}\")\n",
    "    \n",
    "    return documents, total_tables\n",
    "\n",
    "# Process all PDFs\n",
    "raw_documents, total_tables = ingest_pdfs_with_docling(sample_docs_dir, conn)\n",
    "\n",
    "# Commit tables to database\n",
    "conn.commit()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"‚úì PDF Processing Complete!\")\n",
    "print(f\"  ‚Ä¢ Documents processed: {len(raw_documents)}\")\n",
    "print(f\"  ‚Ä¢ Tables extracted to SQL: {total_tables}\")\n",
    "print(f\"  ‚Ä¢ Database: {db_path}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Display sample if documents exist\n",
    "if raw_documents:\n",
    "    print(\"\\n--- Sample Document ---\")\n",
    "    print(f\"Source: {raw_documents[0].metadata['filename']}\")\n",
    "    print(f\"Type: {raw_documents[0].metadata['type']}\")\n",
    "    print(f\"Tables: {raw_documents[0].metadata['tables_extracted']}\")\n",
    "    print(f\"Content preview:\\n{raw_documents[0].page_content[:300]}...\")\n",
    "else:\n",
    "    print(\"\\n‚ö† No documents were successfully processed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üìä SQL TABLE QUERYING DEMONSTRATION\n",
      "============================================================\n",
      "\n",
      "‚úì Found 53 tables in database:\n",
      "\n",
      "\n",
      "üìã Table: placement_report_2023_24_table_1\n",
      "------------------------------------------------------------\n",
      "                               Particulars Page_No_\n",
      "                              Introduction        3\n",
      "                       Highlights in Brief        3\n",
      "                       Student Preparation        4\n",
      "Recruiter's Profile and Overall Statistics        4\n",
      "                   Program-wise statistics        6\n",
      "\n",
      "Total rows: 17\n",
      "\n",
      "üìã Table: placement_report_2023_24_table_2\n",
      "------------------------------------------------------------\n",
      "                                    Key_Metric Details\n",
      "           Total Number of Registered Students    2414\n",
      "Total Number of Actively Participated Students    1979\n",
      "               Total Number of Accepted Offers    1475\n",
      "        Total Number of Companies Offered Jobs     364\n",
      "                    Total Number of Job Offers    1650\n",
      "\n",
      "Total rows: 11\n",
      "\n",
      "üìã Table: placement_report_2023_24_table_3\n",
      "------------------------------------------------------------\n",
      "                             0    1\n",
      "       Placement Outside India   78\n",
      "   Placements in MNCs in India  775\n",
      "Placements in Indian Companies  622\n",
      "                         Total 1475\n",
      "\n",
      "Total rows: 4\n",
      "\n",
      "üìã Table: placement_report_2023_24_table_4\n",
      "------------------------------------------------------------\n",
      "                        Program Registered Participated Placed Placement_Percentage\n",
      "                        B.Tech.        930          819    683                83.39\n",
      "                       M. Tech.        564          497    415                 83.5\n",
      "                M.S. (Research)         23           15     14                93.33\n",
      "Dual Degree (B.Tech. + M.Tech.)        216          192    152                79.16\n",
      "                M.Sc. (2 years)        228          158     87                55.06\n",
      "\n",
      "Total rows: 14\n",
      "\n",
      "üìã Table: placement_report_2023_24_table_5\n",
      "------------------------------------------------------------\n",
      "Academic_Year Number_of_companies_recruiting_Campus_Placements\n",
      "   2016- 2017                                              294\n",
      "   2017- 2018                                              322\n",
      "  2019 - 2020                                              313\n",
      "  2020 - 2021                                              292\n",
      "  2021 - 2022                                              332\n",
      "\n",
      "Total rows: 7\n",
      "\n",
      "üìã Table: placement_report_2023_24_table_6\n",
      "------------------------------------------------------------\n",
      "    Department_/_Centers_/_Program Registered Participated Placed\n",
      "             Aerospace Engineering        140          119     76\n",
      "Applied Statistics and Informatics         34           30     24\n",
      "                     Earth Science         61           34     14\n",
      "            Biomedical Engineering         21           15     12\n",
      "    Biosciences and Bioengineering         34           21     11\n",
      "\n",
      "Total rows: 30\n",
      "\n",
      "üìã Table: placement_report_2023_24_table_7\n",
      "------------------------------------------------------------\n",
      "                  Sector Number_of_Selections Number_of_companies\n",
      "Engineering & Technology                  430                 106\n",
      "           IT / Software                  307                  84\n",
      "              Consulting                  117                  29\n",
      "                 Finance                  113                  33\n",
      "  Research & Development                   97                  36\n",
      "\n",
      "Total rows: 15\n",
      "\n",
      "üìã Table: placement_report_2023_24_table_8\n",
      "------------------------------------------------------------\n",
      "Range_of_gross_salary_(in_INR)_(Lakhs_per_annum) Number_of_Offers Number_of_Companies\n",
      "                                        Above 20              558                 123\n",
      "                             Between 16.75 to 20              230                  70\n",
      "                             Between 14 to 16.75              227                  60\n",
      "                                Between 12 to 14               93                  29\n",
      "                                Between 10 to 12              161                  46\n",
      "\n",
      "Total rows: 9\n",
      "\n",
      "üìã Table: placement_report_2023_24_table_9\n",
      "------------------------------------------------------------\n",
      "                                    Particulars                                                 Statistics_(2024)\n",
      "Number of Pre-placements offers received (PPOs)                                                               300\n",
      "           Pre-placement offers (PPOs) accepted                                                               258\n",
      "              Total number of internship offers                                                              1267\n",
      "                      Type of internship offers 1,177 offers made by companies and 90 offers made by universities\n",
      "\n",
      "Total rows: 4\n",
      "\n",
      "üìã Table: placement_report_2023_24_table_10\n",
      "------------------------------------------------------------\n",
      "     Academic_Year 2023___2024 2022___2023 2021___2022 2020___2021\n",
      "Total Participated         118         131          67          76\n",
      "      Total Placed          32          41          30          14\n",
      "\n",
      "Total rows: 2\n",
      "\n",
      "üìã Table: placement_report_2023_24_table_11\n",
      "------------------------------------------------------------\n",
      "                         Description 2023___24 2022___23\n",
      "        Median Salary (CTC, in LPA*)     17.92     16.66\n",
      "               Average CTC (in LPA*)     23.50     21.82\n",
      "Total Number of International Offers        76        65\n",
      "Total Number of Pre-Placement Offers       258       194\n",
      "\n",
      "Total rows: 4\n",
      "\n",
      "üìã Table: placement_report_2023_24_table_12\n",
      "------------------------------------------------------------\n",
      "                              0         1         2         3         4\n",
      "                        Program 2020 - 21 2021 - 22 2022 - 23 2023 - 24\n",
      "                        B.Tech.       468       544       598       683\n",
      "Dual Degree (B.Tech. + M.Tech.)       132       184       178       152\n",
      "                        M.Tech.       406       523       479       415\n",
      "                         Others       144       190       261       225\n",
      "\n",
      "Total rows: 6\n",
      "\n",
      "üìã Table: placement_report_2023_24_table_13\n",
      "------------------------------------------------------------\n",
      "                 Sectors B__Tech_ Dual_Degree M__Tech_ Other_Programs Total\n",
      "Engineering & Technology      159          46      194             31   430\n",
      "             IT/Software      178          22       78             29   307\n",
      "                 Finance       80          14        3             16   113\n",
      "              Consulting       65          15       23             14   117\n",
      "  Research & Development       19          15       39             24    97\n",
      "\n",
      "Total rows: 17\n",
      "\n",
      "üìã Table: placement_report_2023_24_table_14\n",
      "------------------------------------------------------------\n",
      "           Department 2020___2021 2021___2022 2022___2023 2023___2024\n",
      "Aerospace Engineering          33          58          54          40\n",
      " Chemical Engineering          98         112         108         138\n",
      "            Chemistry          20          19           7          35\n",
      "    Civil Engineering          49          85          60         118\n",
      "     Computer Science         237         252         273         242\n",
      "\n",
      "Total rows: 12\n",
      "\n",
      "üìã Table: placement_report_2023_24_table_15\n",
      "------------------------------------------------------------\n",
      "                          0                  1                  2\n",
      "Total number of internships Summer internships Winter internships\n",
      "                       1267               1178                 89\n",
      "\n",
      "Total rows: 2\n",
      "\n",
      "üìã Table: placement_report_2023_24_table_16\n",
      "------------------------------------------------------------\n",
      "                          0              1                 2\n",
      "Total number of internships Company offers University offers\n",
      "                       1267           1177                90\n",
      "\n",
      "Total rows: 2\n",
      "\n",
      "üìã Table: placement_report_2023_24_table_17\n",
      "------------------------------------------------------------\n",
      "    Department/Center                         Program Participated Placed %Placed\n",
      "Aerospace Engineering                         B.Tech.           72     48   66.67\n",
      "                                              M.Tech.           37     24   64.86\n",
      "                      Dual Degree (B.Tech. + M.Tech.)            4      3   75.00\n",
      "                                                Ph.D.            6      1   16.67\n",
      "     Department Total                                          119     76   63.87\n",
      "\n",
      "Total rows: 27\n",
      "\n",
      "üìã Table: placement_report_2023_24_table_18\n",
      "------------------------------------------------------------\n",
      "Department/Center                      Program Participated Placed %Placed\n",
      "        Chemistry                         B.S.           19      9   47.37\n",
      "                                         M.Sc.           28     10   35.71\n",
      "                                         Ph.D.            9      1   11.11\n",
      "                  Dual Degree (B.S. + M.Tech.)            2      2  100.00\n",
      " Department total                                        59     22   37.28\n",
      "\n",
      "Total rows: 28\n",
      "\n",
      "üìã Table: placement_report_2023_24_table_19\n",
      "------------------------------------------------------------\n",
      "     Department/Center                         Program Participated Placed %Placed\n",
      "                                               B.Tech.           87     79   90.80\n",
      "                                               M.Tech.          113     92   81.42\n",
      "                                                 Ph.D.           10      2   20.00\n",
      "Electrical Engineering  M.S. by research (Exit Degree)            1      0    0.00\n",
      "Electrical Engineering Dual Degree (B.Tech. + M.Tech.)           69     59   85.51\n",
      "\n",
      "Total rows: 28\n",
      "\n",
      "üìã Table: placement_report_2023_24_table_20\n",
      "------------------------------------------------------------\n",
      "           Department/Center                        Program Participated Placed %Placed\n",
      "    Industrial Design Centre                         B.Des.           17      9   52.94\n",
      "                             Dual Degree (B.Des. + M. Des.)           10      4   40.00\n",
      "                                                     M.Des.           63     36   57.14\n",
      "            Department total                                          90     49   54.44\n",
      "Humanities & Social Sciences               M.A. by Research            3      0       0\n",
      "\n",
      "Total rows: 27\n",
      "\n",
      "üìã Table: placement_report_2023_24_table_21\n",
      "------------------------------------------------------------\n",
      "                     Department/Center                         Program Participated Placed %Placed\n",
      "Systems & Control Engineering (SysCon)                         M.Tech.            5      5   83.33\n",
      "Systems & Control Engineering (SysCon) Dual Degree (B.Tech. + M.Tech.)            1      1     100\n",
      "Systems & Control Engineering (SysCon)                           Ph.D.            3      1   33.33\n",
      "                      Department total                                            9      7   63.64\n",
      "    Technology and Development (CTARA)                         M.Tech.           29     25   86.20\n",
      "\n",
      "Total rows: 8\n",
      "\n",
      "üìã Table: IBEnglish_2025_table_1\n",
      "------------------------------------------------------------\n",
      "                                                                                                                        0                                                                                                                                   1\n",
      "PARTI:THEINSTITUTES ....................................................................................................1           PARTI:THEINSTITUTES ....................................................................................................1\n",
      "                                                                                                                       1.        Indian Institutes of Technology ...........................................................................................3\n",
      "                                                                                                                       2.    Academic Programs .............................................................................................................5\n",
      "                                                                                                                       3. Reservation of Seats .............................................................................................................6\n",
      "                                                                                                                       4.       Defence Service Candidates .................................................................................................8\n",
      "\n",
      "Total rows: 35\n",
      "\n",
      "üìã Table: IBEnglish_2025_table_2\n",
      "------------------------------------------------------------\n",
      "                                                                                                                                                                0                                                                                                                                            1\n",
      "                                                                                                                                                              33.               Queries and Grievances .....................................................................................................57\n",
      "                                                                                                                                                34. Hindi Version                      .....................................................................................................................58\n",
      "                        Annexures .............................................................................................................................59    Annexures .............................................................................................................................59\n",
      "                     Annexure-I: S YLLABI .....................................................................................................................61 Annexure-I: S YLLABI .....................................................................................................................61\n",
      "CHEMISTRY .....................................................................................................................................................63                                                                                                                                             \n",
      "\n",
      "Total rows: 33\n",
      "\n",
      "üìã Table: IBEnglish_2025_table_3\n",
      "------------------------------------------------------------\n",
      "        Zone                                    Institute        City Abbreviation\n",
      "   East Zone   Indian Institute of Technology Bhubaneswar Bhubaneswar       IITBBS\n",
      "   East Zone     Indian Institute of Technology Kharagpur   Kharagpur      IITKGP*\n",
      "   East Zone Indian Institute of Technology (ISM) Dhanbad     Dhanbad     IIT(ISM)\n",
      "   East Zone        Indian Institute of Technology Bhilai      Bhilai        IITBH\n",
      "Central Zone      Indian Institute of Technology Kanpur #      Kanpur        IITK*\n",
      "\n",
      "Total rows: 23\n",
      "\n",
      "üìã Table: IBEnglish_2025_table_4\n",
      "------------------------------------------------------------\n",
      "                           0                                                           1       2\n",
      "                     B.Tech.                                      Bachelor of Technology 4 years\n",
      "                        B.S.                                         Bachelor of Science 4 years\n",
      "                     B.Arch.                                    Bachelor of Architecture 5 years\n",
      "Dual Degree B.Tech.- M.Tech. Dual Degree Bachelor of Technology and Master of Technology 5 years\n",
      "       Dual Degree B.S.-M.S.       Dual Degree Bachelor of Science and Master of Science 5 years\n",
      "\n",
      "Total rows: 9\n",
      "\n",
      "üìã Table: IBEnglish_2025_table_5\n",
      "------------------------------------------------------------\n",
      "Date_of_Examination  May_18,_2025_(Sunday)\n",
      "            Paper 1 09:00 IST to 12:00 IST\n",
      "            Paper 2 14:30 IST to 17:30 IST\n",
      "\n",
      "Total rows: 2\n",
      "\n",
      "üìã Table: IBEnglish_2025_table_7\n",
      "------------------------------------------------------------\n",
      "                                                  0                                     1\n",
      "                            For online registration                  https://jeeadv.ac.in\n",
      "                         Online registration begins April 23, 2025 (Wednesday, 10:00 IST)\n",
      "                         Online registration closes      May 02, 2025 (Friday, 17:00 IST)\n",
      "Last date for fee payment for registered candidates      May 05, 2025 (Monday, 17:00 IST)\n",
      "\n",
      "Total rows: 4\n",
      "\n",
      "üìã Table: IBEnglish_2025_table_8\n",
      "------------------------------------------------------------\n",
      "               0                                   1      2\n",
      "Indian Nationals  Female Candidates (all categories) ‚Çπ 1600\n",
      "Indian Nationals          SC, ST, and PwD Candidates ‚Çπ 1600\n",
      "Indian Nationals                All Other Candidates ‚Çπ 3200\n",
      "                 Female Candidates (GEN and GEN-PwD) ‚Çπ 1600\n",
      "                                      OPEN (GEN-PwD) ‚Çπ 1600\n",
      "\n",
      "Total rows: 8\n",
      "\n",
      "üìã Table: IBEnglish_2025_table_9\n",
      "------------------------------------------------------------\n",
      "                                          0                                          1         2\n",
      "Indian Nationals and OCI/PIO (I) candidates                                        All USD 150 #\n",
      "  Foreign Nationals &OCI/PIO (F) candidates     Candidates Residing in SAARC Countries USD 150 #\n",
      "  Foreign Nationals &OCI/PIO (F) candidates Candidates Residing in Non-SAARC Countries USD 250 #\n",
      "\n",
      "Total rows: 3\n",
      "\n",
      "üìã Table: IBEnglish_2025_table_10\n",
      "------------------------------------------------------------\n",
      "City/Town_IIT_BOMBAY_ZONE Code_IIT_BOMBAY_ZONE\n",
      "                      GOA                     \n",
      "                   Panaji                  101\n",
      "           Margao/Madgoan                  102\n",
      "                  GUJARAT                     \n",
      "                Ahmedabad                  103\n",
      "\n",
      "Total rows: 18\n",
      "\n",
      "üìã Table: IBEnglish_2025_table_11\n",
      "------------------------------------------------------------\n",
      "         City/Town Code\n",
      "         KARNATAKA     \n",
      "          Bagalkot  117\n",
      "Belagavi (Belgaum)  118\n",
      "           Bellary  119\n",
      "         Bengaluru  120\n",
      "\n",
      "Total rows: 18\n",
      "\n",
      "üìã Table: IBEnglish_2025_table_12\n",
      "------------------------------------------------------------\n",
      " City/Town Code\n",
      "Aurangabad  133\n",
      "  Bhandara  134\n",
      "Chandrapur  135\n",
      "     Dhule  136\n",
      "   Jalgaon  137\n",
      "\n",
      "Total rows: 21\n",
      "\n",
      "üìã Table: IBEnglish_2025_table_13\n",
      "------------------------------------------------------------\n",
      "City/Town Code\n",
      "   Wardha  154\n",
      " Yavatmal  155\n",
      "\n",
      "Total rows: 2\n",
      "\n",
      "üìã Table: IBEnglish_2025_table_14\n",
      "------------------------------------------------------------\n",
      "DELHI_NCR_Delhi_(East) 201\n",
      "         Delhi (North) 202\n",
      "         Delhi (South) 203\n",
      "          Delhi (West) 204\n",
      "             Faridabad 205\n",
      "                 Noida 206\n",
      "\n",
      "Total rows: 6\n",
      "\n",
      "üìã Table: IBEnglish_2025_table_15\n",
      "------------------------------------------------------------\n",
      "         0   1\n",
      "     Ajmer 211\n",
      "     Alwar 212\n",
      "  Bhilwara 213\n",
      "   Bikaner 214\n",
      "Hanumangar 215\n",
      "\n",
      "Total rows: 10\n",
      "\n",
      "üìã Table: IBEnglish_2025_table_16\n",
      "------------------------------------------------------------\n",
      "City/Town Code\n",
      "  MIZORAM     \n",
      "   Aizawl  318\n",
      " NAGALAND     \n",
      "   Kohima  319\n",
      "   SIKKIM     \n",
      "\n",
      "Total rows: 46\n",
      "\n",
      "üìã Table: IBEnglish_2025_table_17\n",
      "------------------------------------------------------------\n",
      "  City/Town_Sambalpur    Code_518       col_2\n",
      "          WEST BENGAL WEST BENGAL WEST BENGAL\n",
      "              Asansol         519            \n",
      "           Baharampur         520            \n",
      "(Murshidabad) Burdwan         521            \n",
      "             Durgapur         522            \n",
      "\n",
      "Total rows: 50\n",
      "\n",
      "üìã Table: IBEnglish_2025_table_18\n",
      "------------------------------------------------------------\n",
      "     City/Town Code\n",
      "Tiruchirapalli  640\n",
      "   Tirunelveli  641\n",
      "       Vellore  642\n",
      "     Nagercoil  643\n",
      "      Namakkal  644\n",
      "\n",
      "Total rows: 6\n",
      "\n",
      "üìã Table: IBEnglish_2025_table_19\n",
      "------------------------------------------------------------\n",
      "    City/Town Code\n",
      "       Meerut  726\n",
      "    Moradabad  727\n",
      "Muzaffarnagar  728\n",
      "   Saharanpur  729\n",
      "\n",
      "Total rows: 4\n",
      "\n",
      "üìã Table: IBEnglish_2025_table_20\n",
      "------------------------------------------------------------\n",
      "        0   1\n",
      "Abu Dhabi F01\n",
      "Kathmandu F02\n",
      "\n",
      "Total rows: 2\n",
      "\n",
      "üìã Table: IBEnglish_2025_table_21\n",
      "------------------------------------------------------------\n",
      "                                                                                                   0                                                                     1\n",
      "Copy of candidate responses to be available on the JEE (Advanced) 2025 website: https://jeeadv.ac.in                                    May 22, 2025 (Thursday, 17:00 IST)\n",
      "                                   Online display of provisional answer keys on https://jeeadv.ac.in                                      May 26, 2025 (Monday, 10:00 IST)\n",
      "              Receiving feedback from candidates on provisional answer keys through candidate portal May 26, 2025 (Monday, 10:00 IST) to May 27, 2025 (Tuesday, 17:00 IST)\n",
      "                                         Online display of final answer keys on https://jeeadv.ac.in                                     June 02, 2025 (Monday, 10:00 IST)\n",
      "\n",
      "Total rows: 4\n",
      "\n",
      "üìã Table: IBEnglish_2025_table_22\n",
      "------------------------------------------------------------\n",
      "             Rank_List Minimum_percentage_of_marks_in_each_subject Minimum_percentage_of_aggregate_marks\n",
      "Common rank list (CRL)                                        10.0                                  35.0\n",
      "     GEN-EWS rank list                                         9.0                                  31.5\n",
      "     OBC-NCL rank list                                         9.0                                  31.5\n",
      "          SC rank list                                         5.0                                  17.5\n",
      "          ST rank list                                         5.0                                  17.5\n",
      "\n",
      "Total rows: 11\n",
      "\n",
      "üìã Table: IBEnglish_2025_table_23\n",
      "------------------------------------------------------------\n",
      "                          0                                                                                                  1\n",
      "Portal for AAT Registration                                                                               https://jeeadv.ac.in\n",
      "Online Registration for AAT                          June 02, 2025, (Monday, 10:00 IST) to June 03, 2025, (Tuesday, 17:00 IST)\n",
      " Architecture Aptitude Test June 05, 2025 (Thursday) 09:00 to 12:00 IST (Candidate must reach examination centre by 08:00 IST)\n",
      " Declaration of AAT Results                                                                  June 08, 2025 (Sunday), 17:00 IST\n",
      "\n",
      "Total rows: 4\n",
      "\n",
      "üìã Table: IBEnglish_2025_table_24\n",
      "------------------------------------------------------------\n",
      "Sl__No_        Resolution_No_ Date_of_Notification\n",
      "      1 No.12011/68/93-BCC(C)           13.09.1993\n",
      "      2     No.12011/9/94-BCC           19.10.1994\n",
      "      3     No.12011/7/95-BCC           24.05.1995\n",
      "      4    No.12011/96/94-BCC           09.03.1996\n",
      "      5    No.12011/44/96-BCC           11.12.1996\n",
      "\n",
      "Total rows: 37\n",
      "\n",
      "üìã Table: IBEnglish_2025_table_25\n",
      "------------------------------------------------------------\n",
      "S__No_           Disability Affected_Part_of_Body Diagnosis Permanent_physical_impairment/mental_disability_(i\n",
      "     1 Locomotor disability                     @                                                             \n",
      "     2           Low vision                     #                                                             \n",
      "     3            Blindness             Both Eyes                                                             \n",
      "     4   Hearing impairment                     ¬£                                                             \n",
      "     5   Mental retardation                     X                                                             \n",
      "\n",
      "Total rows: 6\n",
      "\n",
      "üìã Table: IBEnglish_2025_table_26\n",
      "------------------------------------------------------------\n",
      "S__No_                                 Disability Diagnosis Permanent_physical_impairment_/_mental_disability_\n",
      "     1                       Locomotor disability                                                             \n",
      "     2 Visual Impairment (blindness / low vision)                                                             \n",
      "     3                         Hearing impairment                                                             \n",
      "     4             Speech and language disability                                                             \n",
      "     5                    Intellectual disability                                                             \n",
      "\n",
      "Total rows: 7\n",
      "\n",
      "üìã Table: IBEnglish_2025_table_27\n",
      "------------------------------------------------------------\n",
      "                         0                                                                                     1                          2                                     3                                                      4\n",
      "         (Signature& Name)                                                                     (Signature& Name)          (Signature& Name)                     (Signature& Name)                                      (Signature& Name)\n",
      "Orthopedic/ PMR specialist Clinical Psychologist / Rehabilitation Psychologist / Psychiatrist / Special Educator Neurologist (if available) Occupational Therapist (if available) Other Expert, as nominated by the Chairperson (if any)\n",
      "\n",
      "Total rows: 2\n",
      "\n",
      "üìã Table: IBEnglish_2025_table_28\n",
      "------------------------------------------------------------\n",
      "  0   1  2  3   4\n",
      "GEN OBC SC ST PwD\n",
      "\n",
      "Total rows: 1\n",
      "\n",
      "üìã Table: IBEnglish_2025_table_29\n",
      "------------------------------------------------------------\n",
      "                                                       col_0 Programme_Name_Bachelor_of_Technology_(4 BHU ISM_Bh Bbs B D_Dh Gn Goa G_H I Jm_J K_Kgp M Mandi Pkd P R_T col_18 col_19 col_20 Rpr col_22 col_23 col_24 col_25 col_26 col_27 col_28 col_29 col_30 col_31 col_32 col_33 col_34 col_35 col_36\n",
      "                                                    Years) 1                                                         ‚úî                      ‚úî     ‚úî ‚úî                                                                                                                                                  \n",
      "               Aerospace 2 Agricultural and Food Engineering                              Engineering                                             ‚úî                                                                                                                                                    \n",
      "3 Artificial Intelligence 4 Artificial Intelligence and Data                                                                 ‚úî         ‚úî          ‚úî                                                                                                                                                    \n",
      "                                                   Analytics                                                                                      ‚úî                                                                                                                                                    \n",
      "              5 Artificial Intelligence and Data Engineering                                                                                                                                 ‚úî                                                                                                         \n",
      "\n",
      "Total rows: 35\n",
      "\n",
      "üìã Table: IBEnglish_2025_table_30\n",
      "------------------------------------------------------------\n",
      "   57                                                                               Mineral_and_Metallurgical_Engineering col_2         ‚úî    col_4 col_5 col_6 col_7 col_8 col_9 col_10 col_11 col_12 col_13 col_14 col_15 col_16\n",
      "   58                                                                                                Mining ‚úî (AR) Mining   ‚úî ‚úî (AR) (AR)                                      ‚úî          (AR)                                   \n",
      "59 60 Engineering Machinery Engineering Naval Architecture and Ocean Engineering Ocean Engineering and Naval Architecture                 61 62 63                                    ‚úî      ‚úî                                   \n",
      "                                                 Petroleum Engineering Pharmaceutical Engineering & Technology Industrial     ‚úî         ‚úî       64     ‚úî                                                                         \n",
      "   65                                                                       Production and Engineering Space Sciences and                                                                                                        \n",
      "                                                                                                              Engineering                              ‚úî                       ‚úî                                                 \n",
      "\n",
      "Total rows: 30\n",
      "\n",
      "üìã Table: IBEnglish_2025_table_31\n",
      "------------------------------------------------------------\n",
      "col_0                                                     Bachelor_of_Technology_and_MBA_(Dual_Degree)_(5_Ye col_2\n",
      "  114 B.Tech (Artificial Intelligence and Data Science) - MBA in Digital Business Management (IIM Bodh Gaya)      \n",
      "  115             B.Tech (Chemical Engineering) - MBA in Hospital and Health Care Management (IIM Bodh Gaya)      \n",
      "  116  B.Tech (Chemical Science and Technology) - MBA in Hospital and Health Care Management (IIM Bodh Gaya)      \n",
      "  117                                                       B.Tech (Civil Engineering) - MBA (IIM Bodh Gaya)      \n",
      "  118         B.Tech (Computer Science and Engineering) - MBA in Digital Business Management (IIM Bodh Gaya)      \n",
      "\n",
      "Total rows: 16\n",
      "\n",
      "üìã Table: IBEnglish_2025_table_32\n",
      "------------------------------------------------------------\n",
      "Sl__No_                                           Activity                                                  Day,_Date_and_Time_(IST)\n",
      "      1      JEE (Main) 2025 [Computer Based Tests by NTA]                                                        JEE (Main) website\n",
      "      2                  Results of JEE (Main) 2025 by NTA                                                        JEE (Main) website\n",
      "      3        Online Registration for JEE (Advanced) 2025 Wednesday, April 23, 2025 (10:00 IST) to Friday, May 02, 2025 (23:59 IST)\n",
      "      4 Last date for fee payment of registered candidates                                          Monday, May 05, 2025 (23:59 IST)\n",
      "      5               Admit Card available for downloading      Sunday, May 11, 2025 (10:00 IST) to Sunday, May 18, 2025 (14:30 IST)\n",
      "\n",
      "Total rows: 15\n",
      "\n",
      "üìã Table: IBEnglish_2025_table_33\n",
      "------------------------------------------------------------\n",
      "                                      Zone_wise_Institutes                                               Address_for_Correspondence                                                                 Phone_Number_and_Email\n",
      " South IIT Hyderabad* IIT Madras IIT Palakkad IIT Tirupati           Chairperson JEE (Advanced) 2025 IIT Hyderabad Hyderabad-502285                                   Phone: +91 40 23016802 E-mail: office.jee@iith.ac.in\n",
      "                  West IIT Bombay* IIT Gandhinagar IIT Goa        Chairperson JEE (Advanced) 2025 IIT Bombay, Powai Mumbai - 400076                       Phone: +91 22 25769093 +91 22 25764063 E-mail: jeeadv@iitb.ac.in\n",
      "IIT Dharwad North-Central IIT Delhi* IIT Jodhpur IIT Jammu Chairperson, JEE (Advanced) 2025 IIT Delhi, Hauz Khas New Delhi - 110016 Phone: +91 11 26591785 +91 11 26591798 +91 11 26597099 E-mail: jeeadv@admin.iitd.ac.in\n",
      "                        North-East IIT Guwahati* IIT Patna           Chairperson JEE (Advanced) 2025 IIT Guwahati Guwahati - 781039                                          Phone: +91 361 2692795 E-mail: jee@iitg.ac.in\n",
      "        Central IIT Kanpur*# IIT Indore IIT (BHU) Varanasi              Chairperson JEE (Advanced) 2025 IIT Kanpur Kanpur - 208 016                                       Phone: +91 512 6792600 E-mail: jeeadv@iitk.ac.in\n",
      "\n",
      "Total rows: 7\n",
      "\n",
      "============================================================\n",
      "üí° You can now query these tables using SQL!\n",
      "============================================================\n",
      "\n",
      "üìù Example SQL Queries:\n",
      "------------------------------------------------------------\n",
      "\n",
      "# Query 1: Select all from first table\n",
      "SELECT * FROM placement_report_2023_24_table_1;\n",
      "\n",
      "# Query 2: Count rows\n",
      "SELECT COUNT(*) FROM placement_report_2023_24_table_1;\n",
      "\n",
      "# Query 3: Filter data (adjust column name as needed)\n",
      "SELECT * FROM placement_report_2023_24_table_1 WHERE col_0 IS NOT NULL;\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate SQL Table Querying from Extracted PDFs\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä SQL TABLE QUERYING DEMONSTRATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get list of all tables in the database\n",
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "tables = cursor.fetchall()\n",
    "\n",
    "if tables:\n",
    "    print(f\"\\n‚úì Found {len(tables)} tables in database:\\n\")\n",
    "    \n",
    "    for table_name in tables:\n",
    "        table_name = table_name[0]\n",
    "        print(f\"\\nüìã Table: {table_name}\")\n",
    "        print(\"-\"*60)\n",
    "        \n",
    "        # Get table info\n",
    "        cursor.execute(f\"SELECT * FROM {table_name} LIMIT 5\")\n",
    "        rows = cursor.fetchall()\n",
    "        \n",
    "        # Get column names\n",
    "        cursor.execute(f\"PRAGMA table_info({table_name})\")\n",
    "        columns = [col[1] for col in cursor.fetchall()]\n",
    "        \n",
    "        # Display as DataFrame for better formatting\n",
    "        df = pd.DataFrame(rows, columns=columns)\n",
    "        print(df.to_string(index=False))\n",
    "        \n",
    "        # Show row count\n",
    "        cursor.execute(f\"SELECT COUNT(*) FROM {table_name}\")\n",
    "        count = cursor.fetchone()[0]\n",
    "        print(f\"\\nTotal rows: {count}\")\n",
    "        \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üí° You can now query these tables using SQL!\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Example SQL queries\n",
    "    print(\"\\nüìù Example SQL Queries:\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    example_table = tables[0][0]\n",
    "    print(f\"\\n# Query 1: Select all from first table\")\n",
    "    print(f\"SELECT * FROM {example_table};\")\n",
    "    \n",
    "    print(f\"\\n# Query 2: Count rows\")\n",
    "    print(f\"SELECT COUNT(*) FROM {example_table};\")\n",
    "    \n",
    "    print(f\"\\n# Query 3: Filter data (adjust column name as needed)\")\n",
    "    print(f\"SELECT * FROM {example_table} WHERE col_0 IS NOT NULL;\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\n‚ö† No tables were extracted from PDFs\")\n",
    "    print(\"This might mean:\")\n",
    "    print(\"  ‚Ä¢ PDFs don't contain tables\")\n",
    "    print(\"  ‚Ä¢ Table extraction failed\")\n",
    "    print(\"  ‚Ä¢ Tables weren't properly formatted in PDFs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üìä CSV/EXCEL FILE PROCESSING & QUERYING\n",
      "============================================================\n",
      "\n",
      "‚úì Found 4 CSV/Excel files\n",
      "\n",
      "‚úì Loaded: IIT_Bombay_Course_Enrollment.csv (10 rows √ó 6 columns)\n",
      "‚úì Loaded: IIT_Bombay_Research_Publications.csv (10 rows √ó 6 columns)\n",
      "‚úì Loaded: IIT_Bombay_Admissions_2020_2023.csv (12 rows √ó 5 columns)\n",
      "‚úì Loaded: IIT_Bombay_Placement_Stats.csv (16 rows √ó 6 columns)\n",
      "\n",
      "============================================================\n",
      "üìã Dataset: IIT_Bombay_Course_Enrollment\n",
      "============================================================\n",
      "\n",
      "Shape: 10 rows √ó 6 columns\n",
      "Columns: Course_Code, Course_Name, Credits, Enrolled_2023, Pass_Rate, Instructor\n",
      "\n",
      "First 5 rows:\n",
      "Course_Code          Course_Name  Credits  Enrolled_2023  Pass_Rate Instructor\n",
      "      CS101 Intro to Programming        4            450       92.5 Dr. Sharma\n",
      "      CS201      Data Structures        4            380       88.3  Dr. Kumar\n",
      "      CS301     Machine Learning        3            220       85.2  Dr. Mehta\n",
      "      EE101       Circuit Theory        4            320       90.1  Dr. Singh\n",
      "      EE201      Digital Systems        3            250       87.6  Dr. Gupta\n",
      "\n",
      "üìà Statistics for numeric columns:\n",
      "         Credits  Enrolled_2023  Pass_Rate\n",
      "count  10.000000      10.000000  10.000000\n",
      "mean    3.700000     357.000000  89.040000\n",
      "std     0.483046     113.141995   2.159321\n",
      "min     3.000000     210.000000  85.200000\n",
      "25%     3.250000     260.000000  87.775000\n",
      "50%     4.000000     350.000000  89.200000\n",
      "75%     4.000000     465.000000  90.250000\n",
      "max     4.000000     500.000000  92.500000\n",
      "\n",
      "============================================================\n",
      "üìã Dataset: IIT_Bombay_Research_Publications\n",
      "============================================================\n",
      "\n",
      "Shape: 10 rows √ó 6 columns\n",
      "Columns: Faculty_Name, Department, Publications_2023, Citations, H_Index, Research_Area\n",
      "\n",
      "First 5 rows:\n",
      "   Faculty_Name Department  Publications_2023  Citations  H_Index       Research_Area\n",
      " Prof. A. Kumar        CSE                 15        450       18    Machine Learning\n",
      " Prof. B. Singh         EE                 12        380       16       Power Systems\n",
      "Prof. C. Sharma         ME                  8        220       12 Thermal Engineering\n",
      " Prof. D. Patel        CSE                 18        520       21              AI/NLP\n",
      " Prof. E. Verma   Chemical                 10        290       14 Process Engineering\n",
      "\n",
      "üìà Statistics for numeric columns:\n",
      "       Publications_2023   Citations    H_Index\n",
      "count           10.00000   10.000000  10.000000\n",
      "mean            11.70000  335.000000  15.000000\n",
      "std              3.40098  118.743421   3.366502\n",
      "min              7.00000  165.000000  10.000000\n",
      "25%              9.25000  237.500000  12.500000\n",
      "50%             11.50000  360.000000  15.500000\n",
      "75%             13.75000  406.250000  16.750000\n",
      "max             18.00000  520.000000  21.000000\n",
      "\n",
      "============================================================\n",
      "üìã Dataset: IIT_Bombay_Admissions_2020_2023\n",
      "============================================================\n",
      "\n",
      "Shape: 12 rows √ó 5 columns\n",
      "Columns: Year, Department, Admitted_Students, Average_JEE_Rank, Female_Percentage\n",
      "\n",
      "First 5 rows:\n",
      " Year       Department  Admitted_Students  Average_JEE_Rank  Female_Percentage\n",
      " 2020 Computer Science                120                50               18.5\n",
      " 2020       Electrical                110               180               22.3\n",
      " 2020       Mechanical                 95               350               15.2\n",
      " 2021 Computer Science                125                45               19.8\n",
      " 2021       Electrical                115               175               23.5\n",
      "\n",
      "üìà Statistics for numeric columns:\n",
      "              Year  Admitted_Students  Average_JEE_Rank  Female_Percentage\n",
      "count    12.000000           12.00000         12.000000          12.000000\n",
      "mean   2021.500000          114.00000        185.166667          20.541667\n",
      "std       1.167748           13.07322        126.067683           3.453446\n",
      "min    2020.000000           95.00000         40.000000          15.200000\n",
      "25%    2020.750000          101.50000         48.750000          18.250000\n",
      "50%    2021.500000          116.50000        172.500000          20.500000\n",
      "75%    2022.250000          121.25000        331.250000          22.750000\n",
      "max    2023.000000          135.00000        350.000000          26.200000\n",
      "\n",
      "============================================================\n",
      "üìã Dataset: IIT_Bombay_Placement_Stats\n",
      "============================================================\n",
      "\n",
      "Shape: 16 rows √ó 6 columns\n",
      "Columns: Year, Department, Students_Placed, Avg_Package_LPA, Highest_Package_LPA, Placement_Percentage\n",
      "\n",
      "First 5 rows:\n",
      " Year Department  Students_Placed  Avg_Package_LPA  Highest_Package_LPA  Placement_Percentage\n",
      " 2020        CSE              115             24.5                 58.0                  95.8\n",
      " 2020         EE               98             18.2                 45.0                  89.1\n",
      " 2020         ME               85             12.5                 28.0                  89.5\n",
      " 2020      Civil               72             10.8                 22.0                  80.0\n",
      " 2021        CSE              120             26.8                 62.0                  96.0\n",
      "\n",
      "üìà Statistics for numeric columns:\n",
      "              Year  Students_Placed  Avg_Package_LPA  Highest_Package_LPA  Placement_Percentage\n",
      "count    16.000000        16.000000        16.000000            16.000000             16.000000\n",
      "mean   2021.500000        98.000000        18.737500            44.000000             90.406250\n",
      "std       1.154701        18.136519         6.888916            16.629291              4.961649\n",
      "min    2020.000000        72.000000        10.800000            22.000000             80.000000\n",
      "25%    2020.750000        83.750000        13.025000            29.500000             88.325000\n",
      "50%    2021.500000        96.500000        17.000000            41.500000             91.500000\n",
      "75%    2022.250000       109.750000        23.975000            58.000000             93.775000\n",
      "max    2023.000000       130.000000        32.500000            75.000000             96.300000\n",
      "\n",
      "============================================================\n",
      "üîç ADVANCED QUERYING EXAMPLES\n",
      "============================================================\n",
      "\n",
      "üìå Example 1: IIT Bombay Admissions Analysis\n",
      "------------------------------------------------------------\n",
      "\n",
      "Latest year admissions (2023):\n",
      " Year       Department  Admitted_Students  Average_JEE_Rank  Female_Percentage\n",
      " 2023 Computer Science                135                40               22.5\n",
      " 2023       Electrical                120               165               26.2\n",
      " 2023       Mechanical                102               330               18.9\n",
      "\n",
      "Total admissions by department (2020-2023):\n",
      "Department\n",
      "Computer Science    510\n",
      "Electrical          463\n",
      "Mechanical          395\n",
      "\n",
      "\n",
      "üìå Example 2: IIT Bombay Placement Statistics\n",
      "------------------------------------------------------------\n",
      "\n",
      "Highest package offered:\n",
      "  Department: CSE\n",
      "  Year: 2023\n",
      "  Package: ‚Çπ75.0 LPA\n",
      "\n",
      "Average package by department (all years):\n",
      "Department\n",
      "CSE      28.25\n",
      "EE       20.70\n",
      "ME       14.00\n",
      "Civil    12.00\n",
      "\n",
      "\n",
      "üìå Example 3: IIT Bombay Research Analysis\n",
      "------------------------------------------------------------\n",
      "\n",
      "Top 5 researchers by publications (2023):\n",
      "  Faculty_Name Department  Publications_2023  Citations\n",
      "Prof. D. Patel        CSE                 18        520\n",
      "Prof. A. Kumar        CSE                 15        450\n",
      "Prof. F. Gupta    Physics                 14        410\n",
      "  Prof. J. Rao      Civil                 13        395\n",
      "Prof. B. Singh         EE                 12        380\n",
      "\n",
      "\n",
      "üìå Example 4: IIT Bombay Course Enrollment\n",
      "------------------------------------------------------------\n",
      "\n",
      "Top 5 most enrolled courses:\n",
      "Course_Code          Course_Name  Enrolled_2023  Pass_Rate\n",
      "      MA101           Calculus I            500       86.8\n",
      "      PH101            Physics I            480       88.9\n",
      "      CH101          Chemistry I            470       90.3\n",
      "      CS101 Intro to Programming            450       92.5\n",
      "      CS201      Data Structures            380       88.3\n",
      "\n",
      "Courses with highest pass rates:\n",
      "Course_Code           Course_Name  Pass_Rate  Enrolled_2023\n",
      "      CS101  Intro to Programming       92.5            450\n",
      "      ME101 Engineering Mechanics       91.2            290\n",
      "      CH101           Chemistry I       90.3            470\n",
      "      EE101        Circuit Theory       90.1            320\n",
      "      ME201        Thermodynamics       89.5            210\n",
      "\n",
      "============================================================\n",
      "üí° KEY CAPABILITIES DEMONSTRATED:\n",
      "============================================================\n",
      "‚úì Loading CSV/Excel files with pandas\n",
      "‚úì Data filtering and querying\n",
      "‚úì Aggregations (sum, mean, max, etc.)\n",
      "‚úì Grouping and sorting\n",
      "‚úì Statistical analysis\n",
      "‚úì Multi-file data integration\n",
      "\n",
      "üéØ Use Cases for RAG:\n",
      "------------------------------------------------------------\n",
      "‚Ä¢ Natural language queries on structured data\n",
      "‚Ä¢ 'What was the highest package in CSE department?'\n",
      "‚Ä¢ 'Show me admission trends over the years'\n",
      "‚Ä¢ 'Which courses have the best pass rates?'\n",
      "‚Ä¢ 'List top researchers in Machine Learning'\n",
      "\n",
      "‚úì 4 datasets loaded and ready for querying!\n",
      "  These can be used with LangChain SQL agents for natural language querying\n"
     ]
    }
   ],
   "source": [
    "# CSV/Excel File Processing and Querying Demonstration\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä CSV/EXCEL FILE PROCESSING & QUERYING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load all CSV/Excel files\n",
    "csv_excel_files = list(sample_docs_dir.glob(\"*.csv\")) + list(sample_docs_dir.glob(\"*.xlsx\"))\n",
    "\n",
    "if not csv_excel_files:\n",
    "    print(\"\\n‚ö† No CSV/Excel files found!\")\n",
    "else:\n",
    "    dataframes = {}\n",
    "    \n",
    "    print(f\"\\n‚úì Found {len(csv_excel_files)} CSV/Excel files\\n\")\n",
    "    \n",
    "    # Load each file\n",
    "    for file_path in csv_excel_files:\n",
    "        try:\n",
    "            if file_path.suffix == '.csv':\n",
    "                df = pd.read_csv(file_path)\n",
    "            else:  # .xlsx\n",
    "                df = pd.read_excel(file_path)\n",
    "            \n",
    "            dataframes[file_path.stem] = df\n",
    "            print(f\"‚úì Loaded: {file_path.name} ({len(df)} rows √ó {len(df.columns)} columns)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚úó Error loading {file_path.name}: {str(e)}\")\n",
    "    \n",
    "    # Display each dataset\n",
    "    for name, df in dataframes.items():\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(f\"üìã Dataset: {name}\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Show basic info\n",
    "        print(f\"\\nShape: {df.shape[0]} rows √ó {df.shape[1]} columns\")\n",
    "        print(f\"Columns: {', '.join(df.columns)}\")\n",
    "        \n",
    "        # Show first few rows\n",
    "        print(f\"\\nFirst 5 rows:\")\n",
    "        print(df.head().to_string(index=False))\n",
    "        \n",
    "        # Show basic statistics for numeric columns\n",
    "        numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "        if len(numeric_cols) > 0:\n",
    "            print(f\"\\nüìà Statistics for numeric columns:\")\n",
    "            print(df[numeric_cols].describe().to_string())\n",
    "    \n",
    "    # Demonstrate advanced querying capabilities\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üîç ADVANCED QUERYING EXAMPLES\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Example 1: Query IIT Bombay Admissions data\n",
    "    if 'IIT_Bombay_Admissions_2020_2023' in dataframes:\n",
    "        print(\"\\nüìå Example 1: IIT Bombay Admissions Analysis\")\n",
    "        print(\"-\"*60)\n",
    "        df_adm = dataframes['IIT_Bombay_Admissions_2020_2023']\n",
    "        \n",
    "        # Filter by year\n",
    "        latest_year = df_adm['Year'].max()\n",
    "        print(f\"\\nLatest year admissions ({latest_year}):\")\n",
    "        print(df_adm[df_adm['Year'] == latest_year].to_string(index=False))\n",
    "        \n",
    "        # Department with highest admissions\n",
    "        total_by_dept = df_adm.groupby('Department')['Admitted_Students'].sum().sort_values(ascending=False)\n",
    "        print(f\"\\nTotal admissions by department (2020-2023):\")\n",
    "        print(total_by_dept.to_string())\n",
    "    \n",
    "    # Example 2: Query IIT Bombay Placement data\n",
    "    if 'IIT_Bombay_Placement_Stats' in dataframes:\n",
    "        print(\"\\n\\nüìå Example 2: IIT Bombay Placement Statistics\")\n",
    "        print(\"-\"*60)\n",
    "        df_place = dataframes['IIT_Bombay_Placement_Stats']\n",
    "        \n",
    "        # Find department with highest package\n",
    "        max_package_row = df_place.loc[df_place['Highest_Package_LPA'].idxmax()]\n",
    "        print(f\"\\nHighest package offered:\")\n",
    "        print(f\"  Department: {max_package_row['Department']}\")\n",
    "        print(f\"  Year: {max_package_row['Year']}\")\n",
    "        print(f\"  Package: ‚Çπ{max_package_row['Highest_Package_LPA']} LPA\")\n",
    "        \n",
    "        # Average package trend by department\n",
    "        print(f\"\\nAverage package by department (all years):\")\n",
    "        avg_by_dept = df_place.groupby('Department')['Avg_Package_LPA'].mean().sort_values(ascending=False)\n",
    "        print(avg_by_dept.to_string())\n",
    "    \n",
    "    # Example 3: Query IIT Bombay Research data\n",
    "    if 'IIT_Bombay_Research_Publications' in dataframes:\n",
    "        print(\"\\n\\nüìå Example 3: IIT Bombay Research Analysis\")\n",
    "        print(\"-\"*60)\n",
    "        df_research = dataframes['IIT_Bombay_Research_Publications']\n",
    "        \n",
    "        # Top researchers by publications\n",
    "        print(\"\\nTop 5 researchers by publications (2023):\")\n",
    "        top_researchers = df_research.nlargest(5, 'Publications_2023')[['Faculty_Name', 'Department', 'Publications_2023', 'Citations']]\n",
    "        print(top_researchers.to_string(index=False))\n",
    "    \n",
    "    # Example 4: Query Course Enrollment data\n",
    "    if 'IIT_Bombay_Course_Enrollment' in dataframes:\n",
    "        print(\"\\n\\nüìå Example 4: IIT Bombay Course Enrollment\")\n",
    "        print(\"-\"*60)\n",
    "        df_courses = dataframes['IIT_Bombay_Course_Enrollment']\n",
    "        \n",
    "        # Most enrolled courses\n",
    "        print(\"\\nTop 5 most enrolled courses:\")\n",
    "        top_courses = df_courses.nlargest(5, 'Enrolled_2023')[['Course_Code', 'Course_Name', 'Enrolled_2023', 'Pass_Rate']]\n",
    "        print(top_courses.to_string(index=False))\n",
    "        \n",
    "        # Courses by pass rate\n",
    "        print(\"\\nCourses with highest pass rates:\")\n",
    "        high_pass = df_courses.nlargest(5, 'Pass_Rate')[['Course_Code', 'Course_Name', 'Pass_Rate', 'Enrolled_2023']]\n",
    "        print(high_pass.to_string(index=False))\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üí° KEY CAPABILITIES DEMONSTRATED:\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"‚úì Loading CSV/Excel files with pandas\")\n",
    "    print(\"‚úì Data filtering and querying\")\n",
    "    print(\"‚úì Aggregations (sum, mean, max, etc.)\")\n",
    "    print(\"‚úì Grouping and sorting\")\n",
    "    print(\"‚úì Statistical analysis\")\n",
    "    print(\"‚úì Multi-file data integration\")\n",
    "    \n",
    "    print(\"\\nüéØ Use Cases for RAG:\")\n",
    "    print(\"-\"*60)\n",
    "    print(\"‚Ä¢ Natural language queries on structured data\")\n",
    "    print(\"‚Ä¢ 'What was the highest package in CSE department?'\")\n",
    "    print(\"‚Ä¢ 'Show me admission trends over the years'\")\n",
    "    print(\"‚Ä¢ 'Which courses have the best pass rates?'\")\n",
    "    print(\"‚Ä¢ 'List top researchers in Machine Learning'\")\n",
    "    \n",
    "# Save dataframes for later use in RAG\n",
    "if dataframes:\n",
    "    print(f\"\\n‚úì {len(dataframes)} datasets loaded and ready for querying!\")\n",
    "    print(\"  These can be used with LangChain SQL agents for natural language querying\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ADDING METADATA TO DOCUMENTS\n",
      "============================================================\n",
      "\n",
      "‚úì Added metadata to 3 documents\n",
      "\n",
      "Sample metadata:\n",
      "  ‚Ä¢ Document ID: doc_0\n",
      "  ‚Ä¢ Filename: placement_report_2023_24.pdf\n",
      "  ‚Ä¢ Estimated pages: 14\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ADD METADATA - Simple metadata for better tracking\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ADDING METADATA TO DOCUMENTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "enriched_documents = []\n",
    "\n",
    "for idx, doc in enumerate(raw_documents):\n",
    "    # Simple metadata: page number (estimate), document ID, filename\n",
    "    content_length = len(doc.page_content)\n",
    "    estimated_pages = max(1, content_length // 3000)  # ~3000 chars per page\n",
    "    \n",
    "    # Add simple metadata\n",
    "    doc.metadata['doc_id'] = f\"doc_{idx}\"\n",
    "    doc.metadata['page_count'] = estimated_pages\n",
    "    doc.metadata['char_count'] = content_length\n",
    "    \n",
    "    \n",
    "    enriched_documents.append(doc)\n",
    "\n",
    "print(f\"\\n‚úì Added metadata to {len(enriched_documents)} documents\\n\")\n",
    "print(\"Sample metadata:\")\n",
    "print(f\"  ‚Ä¢ Document ID: {enriched_documents[0].metadata['doc_id']}\")\n",
    "print(f\"  ‚Ä¢ Filename: {enriched_documents[0].metadata['filename']}\")\n",
    "print(f\"  ‚Ä¢ Estimated pages: {enriched_documents[0].metadata['page_count']}\")\n",
    "\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Text Chunking <a id='chunking'></a>\n",
    "\n",
    "Now we'll split documents into chunks using RecursiveCharacterTextSplitter. The metadata will automatically be preserved in each chunk!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CHUNKING DOCUMENTS\n",
      "============================================================\n",
      "\n",
      "‚úì Created 835 chunks\n",
      "  ‚Ä¢ Avg size: 355 chars\n",
      "  ‚Ä¢ Min size: 1 chars\n",
      "  ‚Ä¢ Max size: 499 chars\n",
      "\n",
      "--- Sample Chunk ---\n",
      "Content: ## Placement and Internship Report\n",
      "\n",
      "## Academic Year 2023 - 2024\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "Placement Office Indian Institute of Technology, Bombay\n",
      "\n",
      "## Index...\n",
      "\n",
      "Metadata preserved:\n",
      "  ‚Ä¢ Filename: placement_report_2023_24.pdf\n",
      "  ‚Ä¢ Doc ID: doc_0\n",
      "  ‚Ä¢ Page count: 14\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Simple Chunking with RecursiveCharacterTextSplitter\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CHUNKING DOCUMENTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    ")\n",
    "\n",
    "# Split documents (metadata automatically preserved!)\n",
    "chunks = text_splitter.split_documents(enriched_documents)\n",
    "\n",
    "# Show stats\n",
    "chunk_lengths = [len(c.page_content) for c in chunks]\n",
    "print(f\"\\n‚úì Created {len(chunks)} chunks\")\n",
    "print(f\"  ‚Ä¢ Avg size: {sum(chunk_lengths) / len(chunks):.0f} chars\")\n",
    "print(f\"  ‚Ä¢ Min size: {min(chunk_lengths)} chars\")\n",
    "print(f\"  ‚Ä¢ Max size: {max(chunk_lengths)} chars\")\n",
    "\n",
    "# Show sample chunk with metadata\n",
    "print(\"\\n--- Sample Chunk ---\")\n",
    "sample = chunks[0]\n",
    "print(f\"Content: {sample.page_content[:200]}...\")\n",
    "print(f\"\\nMetadata preserved:\")\n",
    "print(f\"  ‚Ä¢ Filename: {sample.metadata['filename']}\")\n",
    "print(f\"  ‚Ä¢ Doc ID: {sample.metadata['doc_id']}\")\n",
    "print(f\"  ‚Ä¢ Page count: {sample.metadata['page_count']}\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Embeddings with Nomic-AI <a id='embeddings'></a>\n",
    "\n",
    "Embeddings convert text into dense vector representations that capture semantic meaning. We'll use **Nomic-AI embeddings** from Hugging Face - optimized specifically for retrieval tasks like RAG.\n",
    "\n",
    "**Model:** `nomic-ai/nomic-embed-text-v1.5` (768 dimensions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m‚ö†Ô∏è  Warning: 'huggingface-cli login' is deprecated. Use 'hf auth login' instead.\u001b[0m\n",
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `hf`CLI if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "The token `hf_rag_token` has been saved to /Users/varunraste/.cache/huggingface/stored_tokens\n",
      "Your token has been saved to /Users/varunraste/.cache/huggingface/token\n",
      "Login successful.\n",
      "The current active token is: `hf_rag_token`\n"
     ]
    }
   ],
   "source": [
    "#!hf auth login --token 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\n",
    "!huggingface-cli login --token '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'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Initialize Embeddings\n",
    "\n",
    "**Nomic-AI Embeddings** from Hugging Face:\n",
    "- Model: `nomic-ai/nomic-embed-text-v1.5`\n",
    "- Dimensions: 768\n",
    "- Context length: 8192 tokens\n",
    "- License: Open source\n",
    "\n",
    "**Note**: First run may download the model (~500MB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Loading Embeddings from Hugging Face...\n",
      "============================================================\n",
      "\n",
      "üì• Loading: nomic-ai/nomic-embed-text-v1.5\n",
      "   First run downloads ~500MB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Nomic-AI loaded successfully! (768 dims)\n",
      "\n",
      "============================================================\n",
      "‚úÖ Using: nomic-ai/nomic-embed-text-v1.5\n",
      "============================================================\n",
      "\n",
      "üìå DEMO 1: Text to Vector\n",
      "------------------------------------------------------------\n",
      "Input: 'Machine learning is AI'\n",
      "Output: Vector with 768 dimensions\n",
      "Sample values: [0.01440813485532999, 0.018333742395043373, -0.1105121523141861, -0.002919586841017008, 0.06736194342374802]\n",
      "\n",
      "üìå DEMO 2: Semantic Similarity\n",
      "------------------------------------------------------------\n",
      "\n",
      "Query: 'What is machine learning?'\n",
      "\n",
      "Ranked by similarity:\n",
      "\n",
      "1. [0.798] ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   Machine learning uses algorithms\n",
      "\n",
      "2. [0.745] ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   Deep learning is ML subset\n",
      "\n",
      "3. [0.445] ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   Python is a programming language\n",
      "\n",
      "4. [0.317] ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   Weather is sunny today\n",
      "\n",
      "‚úÖ Embeddings ready for RAG!\n"
     ]
    }
   ],
   "source": [
    "# Load Embeddings from Hugging Face (with automatic fallback)\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "import numpy as np\n",
    "\n",
    "print(\"üîß Loading Embeddings from Hugging Face...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Try Nomic-AI first, use fallback if needed\n",
    "try:\n",
    "    print(\"\\nüì• Loading: nomic-ai/nomic-embed-text-v1.5\")\n",
    "    print(\"   First run downloads ~500MB\\n\")\n",
    "    \n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"nomic-ai/nomic-embed-text-v1.5\",\n",
    "        model_kwargs={'device': 'cpu', 'trust_remote_code': True},\n",
    "        encode_kwargs={'normalize_embeddings': True}\n",
    "    )\n",
    "    \n",
    "    # Test it works\n",
    "    _ = embeddings.embed_query(\"test\")\n",
    "    print(\"‚úì Nomic-AI loaded successfully! (768 dims)\\n\")\n",
    "    MODEL_NAME = \"nomic-ai/nomic-embed-text-v1.5\"\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö† Nomic-AI failed: {str(e)[:100]}\")\n",
    "    print(\"\\nüîÑ Using fallback: all-MiniLM-L6-v2\\n\")\n",
    "    \n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        model_kwargs={'device': 'cpu'},\n",
    "        encode_kwargs={'normalize_embeddings': True}\n",
    "    )\n",
    "    \n",
    "    print(\"‚úì Fallback loaded! (384 dims)\\n\")\n",
    "    MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"‚úÖ Using: {MODEL_NAME}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# DEMO 1: Basic Embedding\n",
    "print(\"\\nüìå DEMO 1: Text to Vector\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "text = \"Machine learning is AI\"\n",
    "emb = embeddings.embed_query(text)\n",
    "print(f\"Input: '{text}'\")\n",
    "print(f\"Output: Vector with {len(emb)} dimensions\")\n",
    "print(f\"Sample values: {emb[:5]}\")\n",
    "\n",
    "# DEMO 2: Semantic Similarity\n",
    "print(\"\\nüìå DEMO 2: Semantic Similarity\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "docs = [\n",
    "    \"Machine learning uses algorithms\",\n",
    "    \"Deep learning is ML subset\", \n",
    "    \"Weather is sunny today\",\n",
    "    \"Python is a programming language\"\n",
    "]\n",
    "\n",
    "query = \"What is machine learning?\"\n",
    "print(f\"\\nQuery: '{query}'\\n\")\n",
    "\n",
    "q_emb = embeddings.embed_query(query)\n",
    "d_embs = embeddings.embed_documents(docs)\n",
    "\n",
    "# Calculate similarities\n",
    "sims = [np.dot(q_emb, d) for d in d_embs]\n",
    "ranked = sorted(zip(docs, sims), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Ranked by similarity:\\n\")\n",
    "for i, (doc, score) in enumerate(ranked, 1):\n",
    "    bar = \"‚ñà\" * int(score * 40)\n",
    "    print(f\"{i}. [{score:.3f}] {bar}\")\n",
    "    print(f\"   {doc}\\n\")\n",
    "\n",
    "print(\"‚úÖ Embeddings ready for RAG!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Generation with Ollama <a id='generation'></a>\n",
    "\n",
    "Use a local LLM via Ollama to generate responses based on retrieved context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Ollama LLM...\n",
      "Model: llama3.2:3b (fast & efficient for RAG)\n",
      "Make sure you've pulled it: ollama pull llama3.2:3b\n",
      "\n",
      "Testing LLM with a simple query...\n",
      "‚úì LLM working!\n",
      "\n",
      "Response preview: In the context of Artificial Intelligence (AI), RAG stands for \"Reinforcement Actor-Guided\" or \"Reinforcement Actor-Critic\". However, I found that the...\n"
     ]
    }
   ],
   "source": [
    "# Initialize Ollama LLM for Generation\n",
    "\n",
    "print(\"Initializing Ollama LLM...\")\n",
    "print(\"Model: llama3.2:3b (fast & efficient for RAG)\")\n",
    "print(\"Make sure you've pulled it: ollama pull llama3.2:3b\\n\")\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"llama3.2:3b\",  # Fast model for demos (alternatives: qwen2.5:7b, llama3.1:8b)\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    temperature=0.1,  # Lower temperature for factual responses\n",
    ")\n",
    "\n",
    "# Test the LLM\n",
    "print(\"Testing LLM with a simple query...\")\n",
    "response = llm.invoke(\"What is RAG in AI?\")\n",
    "print(f\"‚úì LLM working!\\n\")\n",
    "print(f\"Response preview: {response.content[:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Vector Store Setup <a id='vectorstore'></a>\n",
    "\n",
    "We'll use FAISS, a lightweight and easy-to-use vector database.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating FAISS vector store...\n",
      "‚úì Created and saved vector store with 835 chunks\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "print(\"Creating FAISS vector store...\")\n",
    "\n",
    "# Create vector store from chunks (includes all metadata automatically)\n",
    "vectorstore = FAISS.from_documents(documents=chunks, embedding=embeddings)\n",
    "\n",
    "# Save to disk\n",
    "vectorstore.save_local(\"./faiss_vectorstore\")\n",
    "\n",
    "print(f\"‚úì Created and saved vector store with {len(chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Table data into Documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Unified Table Ingestion\n",
    "\n",
    "**Challenge**: How to make structured table data searchable semantically?\n",
    "\n",
    "**Solution**: Convert tables to text documents with rich descriptions\n",
    "1. **LLM generates description** of each table/CSV\n",
    "2. **Row-wise data** appended with description\n",
    "3. **Embedded as documents** alongside text chunks\n",
    "\n",
    "**Benefits:**\n",
    "- Single unified search (no separate SQL queries needed)\n",
    "- Semantic understanding of table contents\n",
    "- LLM-generated descriptions improve retrieval accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "UNIFIED TABLE INGESTION: SQL + CSV\n",
      "============================================================\n",
      "\n",
      "SQL Tables:\n",
      "  placement_report_2023_24_table_1... ‚úì\n",
      "  placement_report_2023_24_table_2... ‚úì\n",
      "  placement_report_2023_24_table_3... ‚úì\n",
      "  placement_report_2023_24_table_4... ‚úì\n",
      "  placement_report_2023_24_table_5... ‚úì\n",
      "  placement_report_2023_24_table_6... ‚úì\n",
      "  placement_report_2023_24_table_7... ‚úì\n",
      "  placement_report_2023_24_table_8... ‚úì\n",
      "  placement_report_2023_24_table_9... ‚úì\n",
      "  placement_report_2023_24_table_10... ‚úì\n",
      "  placement_report_2023_24_table_11... ‚úì\n",
      "  placement_report_2023_24_table_12... ‚úì\n",
      "  placement_report_2023_24_table_13... ‚úì\n",
      "  placement_report_2023_24_table_14... ‚úì\n",
      "  placement_report_2023_24_table_15... ‚úì\n",
      "  placement_report_2023_24_table_16... ‚úì\n",
      "  placement_report_2023_24_table_17... ‚úì\n",
      "  placement_report_2023_24_table_18... ‚úì\n",
      "  placement_report_2023_24_table_19... ‚úì\n",
      "  placement_report_2023_24_table_20... ‚úì\n",
      "  placement_report_2023_24_table_21... ‚úì\n",
      "  IBEnglish_2025_table_1... ‚úì\n",
      "  IBEnglish_2025_table_2... ‚úì\n",
      "  IBEnglish_2025_table_3... ‚úì\n",
      "  IBEnglish_2025_table_4... ‚úì\n",
      "  IBEnglish_2025_table_5... ‚úì\n",
      "  IBEnglish_2025_table_7... ‚úì\n",
      "  IBEnglish_2025_table_8... ‚úì\n",
      "  IBEnglish_2025_table_9... ‚úì\n",
      "  IBEnglish_2025_table_10... ‚úì\n",
      "  IBEnglish_2025_table_11... ‚úì\n",
      "  IBEnglish_2025_table_12... ‚úì\n",
      "  IBEnglish_2025_table_13... ‚úì\n",
      "  IBEnglish_2025_table_14... ‚úì\n",
      "  IBEnglish_2025_table_15... ‚úì\n",
      "  IBEnglish_2025_table_16... ‚úì\n",
      "  IBEnglish_2025_table_17... ‚úì\n",
      "  IBEnglish_2025_table_18... ‚úì\n",
      "  IBEnglish_2025_table_19... ‚úì\n",
      "  IBEnglish_2025_table_20... ‚úì\n",
      "  IBEnglish_2025_table_21... ‚úì\n",
      "  IBEnglish_2025_table_22... ‚úì\n",
      "  IBEnglish_2025_table_23... ‚úì\n",
      "  IBEnglish_2025_table_24... ‚úì\n",
      "  IBEnglish_2025_table_25... ‚úì\n",
      "  IBEnglish_2025_table_26... ‚úì\n",
      "  IBEnglish_2025_table_27... ‚úì\n",
      "  IBEnglish_2025_table_28... ‚úì\n",
      "  IBEnglish_2025_table_29... ‚úì\n",
      "  IBEnglish_2025_table_30... ‚úì\n",
      "  IBEnglish_2025_table_31... ‚úì\n",
      "  IBEnglish_2025_table_32... ‚úì\n",
      "  IBEnglish_2025_table_33... ‚úì\n",
      "\n",
      "CSV Files:\n",
      "  IIT_Bombay_Course_Enrollment.csv... ‚úì\n",
      "  IIT_Bombay_Research_Publications.csv... ‚úì\n",
      "  IIT_Bombay_Admissions_2020_2023.csv... ‚úì\n",
      "  IIT_Bombay_Placement_Stats.csv... ‚úì\n",
      "\n",
      "‚úì Total: 184 documents\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from langchain.schema import Document\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"UNIFIED TABLE INGESTION: SQL + CSV\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def generate_description_with_llm(source_name: str, df: pd.DataFrame, llm) -> str:\n",
    "    sample = df.to_string(index=False) if len(df) <= 10 else df.head(10).to_string(index=False)\n",
    "    prompt = f\"\"\"Describe this table comprehensively for semantic search (2-3 sentences). Include ALL categories/types mentioned:\n",
    "Source: {source_name}\n",
    "Columns: {', '.join(df.columns)}\n",
    "Sample: {sample}\n",
    "Description:\"\"\"\n",
    "    return llm.invoke(prompt).content.strip()\n",
    "\n",
    "def ingest_all_tables(db_path, csv_dir, llm, rows_per_doc=5):\n",
    "    all_docs = []\n",
    "    \n",
    "    # SQL tables\n",
    "    print(\"\\nSQL Tables:\")\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "    for (table_name,) in cursor.fetchall():\n",
    "        df = pd.read_sql_query(f\"SELECT * FROM {table_name}\", conn)\n",
    "        if df.empty: continue\n",
    "        print(f\"  {table_name}...\", end=\"\")\n",
    "        desc = generate_description_with_llm(table_name, df, llm)\n",
    "        for i in range(0, len(df), rows_per_doc):\n",
    "            chunk = df.iloc[i:i+rows_per_doc]\n",
    "            all_docs.append(Document(\n",
    "                page_content=f\"Source: {table_name}\\nDescription: {desc}\\nColumns: {', '.join(df.columns)}\\nData:\\n{chunk.to_string(index=False)}\",\n",
    "                metadata={\"type\": \"table_data\", \"source\": table_name}\n",
    "            ))\n",
    "        print(f\" ‚úì\")\n",
    "    conn.close()\n",
    "    \n",
    "    # CSV files\n",
    "    print(\"\\nCSV Files:\")\n",
    "    for file in list(csv_dir.glob(\"*.csv\")) + list(csv_dir.glob(\"*.xlsx\")):\n",
    "        df = pd.read_csv(file) if file.suffix == '.csv' else pd.read_excel(file)\n",
    "        if df.empty: continue\n",
    "        print(f\"  {file.name}...\", end=\"\")\n",
    "        desc = generate_description_with_llm(file.name, df, llm)\n",
    "        for i in range(0, len(df), rows_per_doc):\n",
    "            chunk = df.iloc[i:i+rows_per_doc]\n",
    "            all_docs.append(Document(\n",
    "                page_content=f\"Source: {file.name}\\nDescription: {desc}\\nColumns: {', '.join(df.columns)}\\nData:\\n{chunk.to_string(index=False)}\",\n",
    "                metadata={\"type\": \"table_data\", \"source\": file.name}\n",
    "            ))\n",
    "        print(f\" ‚úì\")\n",
    "    \n",
    "    return all_docs\n",
    "\n",
    "# Run it\n",
    "sample_docs_dir = Path(\"sample_documents\")\n",
    "all_table_documents = ingest_all_tables(\"pdf_tables.db\", sample_docs_dir, llm, 5)\n",
    "print(f\"\\n‚úì Total: {len(all_table_documents)} documents\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Adding table documents...\n",
      "  ‚úì Added 184 documents to memory\n",
      "\n",
      "Step 2: Saving to disk...\n",
      "  ‚úì Saved to ./faiss_vectorstore\n",
      "\n",
      "Step 3: Reloading from disk...\n",
      "  ‚úì Reloaded!\n",
      "\n",
      "Step 4: Verification...\n",
      "  ‚Ä¢ Table documents: 1\n",
      "  ‚Ä¢ Text documents: 9\n",
      "\n",
      "‚úÖ SUCCESS! Tables are now in vector store!\n"
     ]
    }
   ],
   "source": [
    "# Add tables, save, and reload vector store\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "print(\"Step 1: Adding table documents...\")\n",
    "vectorstore.add_documents(all_table_documents)\n",
    "print(f\"  ‚úì Added {len(all_table_documents)} documents to memory\")\n",
    "\n",
    "print(\"\\nStep 2: Saving to disk...\")\n",
    "vectorstore.save_local(\"./faiss_vectorstore\")\n",
    "print(\"  ‚úì Saved to ./faiss_vectorstore\")\n",
    "\n",
    "print(\"\\nStep 3: Reloading from disk...\")\n",
    "vectorstore = FAISS.load_local(\n",
    "    \"./faiss_vectorstore\",\n",
    "    embeddings,\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "print(\"  ‚úì Reloaded!\")\n",
    "\n",
    "print(\"\\nStep 4: Verification...\")\n",
    "test = vectorstore.similarity_search(\"fees\", k=10)\n",
    "table_count = sum(1 for d in test if d.metadata.get('type') == 'table_data')\n",
    "text_count = len(test) - table_count\n",
    "print(f\"  ‚Ä¢ Table documents: {table_count}\")\n",
    "print(f\"  ‚Ä¢ Text documents: {text_count}\")\n",
    "\n",
    "if table_count > 0:\n",
    "    print(\"\\n‚úÖ SUCCESS! Tables are now in vector store!\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Still failed - something else is wrong\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Retrieval <a id='retrieval'></a>\n",
    "\n",
    "Retrieve the most relevant chunks for a given query using similarity search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: 'Tell me about placement statistics for PhD Placement Details for year 2023-24'\n",
      "Retrieving top 3 chunks...\n",
      "\n",
      "--- Result 1 ---\n",
      "Source: placement_report_2023_24.pdf\n",
      "Content: | PhD Placement Details                                         |         11 |\n",
      "| Average Salary, International and Pre-Placement Offer Details |         11 |\n",
      "| Year-wise comparison of placement       ...\n",
      "\n",
      "--- Result 2 ---\n",
      "Source: Unknown\n",
      "Content: Source: placement_report_2023_24_table_1\n",
      "Description: This table provides an overview of the placement report for the academic year 2023-24, covering various categories such as program-wise statistics...\n",
      "\n",
      "--- Result 3 ---\n",
      "Source: Unknown\n",
      "Content: Source: placement_report_2023_24_table_12\n",
      "Description: This table provides a comprehensive overview of the placement statistics for various programs and categories, including:\n",
      "\n",
      "* Program types: B.Tech...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test basic retrieval\n",
    "def test_retrieval(query: str, k: int = 3):\n",
    "    \"\"\"\n",
    "    Test retrieval for a given query.\n",
    "    \n",
    "    Args:\n",
    "        query: The search query\n",
    "        k: Number of documents to retrieve\n",
    "    \"\"\"\n",
    "    print(f\"\\nQuery: '{query}'\")\n",
    "    print(f\"Retrieving top {k} chunks...\\n\")\n",
    "    \n",
    "    # Similarity search\n",
    "    results = vectorstore.similarity_search(query, k=k)\n",
    "    \n",
    "    for i, doc in enumerate(results, 1):\n",
    "        print(f\"--- Result {i} ---\")\n",
    "        print(f\"Source: {doc.metadata.get('filename', 'Unknown')}\")\n",
    "        print(f\"Content: {doc.page_content[:200]}...\")\n",
    "        print()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test with different queries\n",
    "test_queries = [\n",
    "    \"Tell me about placement statistics for PhD Placement Details for year 2023-24\"\n",
    "]\n",
    "\n",
    "# Test first query\n",
    "retrieved_docs = test_retrieval(test_queries[0], k=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: 'Tell me about placement statistics for PhD Placement Details for year 2023-24'\n",
      "Retrieving top 3 chunks with scores...\n",
      "\n",
      "--- Result 1 (Score: 0.3007) ---\n",
      "Source: placement_report_2023_24.pdf\n",
      "Content: | PhD Placement Details                                         |         11 |\n",
      "| Average Salary, International and Pre-Placement Offer Details |      ...\n",
      "\n",
      "--- Result 2 (Score: 0.3122) ---\n",
      "Source: Unknown\n",
      "Content: Source: placement_report_2023_24_table_1\n",
      "Description: This table provides an overview of the placement report for the academic year 2023-24, covering ...\n",
      "\n",
      "--- Result 3 (Score: 0.3290) ---\n",
      "Source: Unknown\n",
      "Content: Source: placement_report_2023_24_table_12\n",
      "Description: This table provides a comprehensive overview of the placement statistics for various programs a...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Retrieval with similarity scores\n",
    "def test_retrieval_with_scores(query: str, k: int = 3):\n",
    "    \"\"\"\n",
    "    Test retrieval with similarity scores.\n",
    "    \"\"\"\n",
    "    print(f\"\\nQuery: '{query}'\")\n",
    "    print(f\"Retrieving top {k} chunks with scores...\\n\")\n",
    "    \n",
    "    # Similarity search with scores\n",
    "    results = vectorstore.similarity_search_with_score(query, k=k)\n",
    "    \n",
    "    for i, (doc, score) in enumerate(results, 1):\n",
    "        print(f\"--- Result {i} (Score: {score:.4f}) ---\")\n",
    "        print(f\"Source: {doc.metadata.get('filename', 'Unknown')}\")\n",
    "        print(f\"Content: {doc.page_content[:150]}...\")\n",
    "        print()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test with scores\n",
    "results_with_scores = test_retrieval_with_scores(test_queries[0], k=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Reranking <a id='reranking'></a>\n",
    "\n",
    "Reranking improves retrieval quality by using more sophisticated models to reorder results. We'll demonstrate both Cohere and BGE rerankers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "INITIALIZING RERANKER\n",
      "============================================================\n",
      "Loading reranker model: cross-encoder/ms-marco-MiniLM-L-6-v2...\n",
      "‚úì Reranker loaded!\n",
      "\n",
      "Testing reranker...\n",
      "‚úì Reranked 3 docs to top 2\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "from langchain.schema import Document\n",
    "from typing import List\n",
    "\n",
    "# Simple Cross-Encoder Reranker\n",
    "class SimpleReranker:\n",
    "    \"\"\"\n",
    "    Simple reranker using cross-encoder model.\n",
    "    Runs locally without requiring API keys.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"):\n",
    "        print(f\"Loading reranker model: {model_name}...\")\n",
    "        self.model = CrossEncoder(model_name)\n",
    "        print(\"‚úì Reranker loaded!\\n\")\n",
    "    \n",
    "    def rerank(self, query: str, documents: List[Document], top_k: int = 3) -> List[Document]:\n",
    "        \"\"\"Rerank documents based on relevance to query\"\"\"\n",
    "        # Create query-document pairs\n",
    "        pairs = [[query, doc.page_content] for doc in documents]\n",
    "        \n",
    "        # Get relevance scores\n",
    "        scores = self.model.predict(pairs)\n",
    "        \n",
    "        # Sort by score (descending)\n",
    "        doc_score_pairs = list(zip(documents, scores))\n",
    "        doc_score_pairs.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Return top_k\n",
    "        return [doc for doc, score in doc_score_pairs[:top_k]]\n",
    "\n",
    "# Initialize reranker\n",
    "print(\"=\"*60)\n",
    "print(\"INITIALIZING RERANKER\")\n",
    "print(\"=\"*60)\n",
    "reranker = SimpleReranker()\n",
    "\n",
    "# Quick test\n",
    "print(\"Testing reranker...\")\n",
    "query = \"machine learning research\"\n",
    "test_docs = [\n",
    "    Document(page_content=\"Machine learning is a subset of AI\"),\n",
    "    Document(page_content=\"The weather is sunny today\"),\n",
    "    Document(page_content=\"Deep learning uses neural networks\")\n",
    "]\n",
    "reranked = reranker.rerank(query, test_docs, top_k=2)\n",
    "print(f\"‚úì Reranked {len(test_docs)} docs to top {len(reranked)}\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. ü§ñ Complete RAG Pipeline\n",
    "\n",
    "Integrate all components into a unified system with smart query routing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì RAG Prompt Template Configured\n",
      "  ‚Ä¢ Clear, direct instructions\n",
      "  ‚Ä¢ No over-conservative guardrails\n",
      "  ‚Ä¢ Will answer when data is present\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "RAG_PROMPT = PromptTemplate(\n",
    "    template=\"\"\"You are a helpful assistant for IIT Bombay information.\n",
    "\n",
    "Context from documents and tables:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Instructions:\n",
    "1. Answer the question directly using information from the context above\n",
    "2. If the context contains the answer, provide it clearly and concisely\n",
    "3. For data/statistics questions, cite the specific numbers/values from tables\n",
    "4. If multiple categories exist (e.g., SAARC vs Non-SAARC), list them all\n",
    "5. Only say \"I don't have this information\" if the context truly doesn't contain relevant information\n",
    "6. Always cite sources: mention the document or table name\n",
    "\n",
    "Answer:\"\"\",\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "print(\"‚úì RAG Prompt Template Configured\")\n",
    "print(\"  ‚Ä¢ Clear, direct instructions\")\n",
    "print(\"  ‚Ä¢ No over-conservative guardrails\")\n",
    "print(\"  ‚Ä¢ Will answer when data is present\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now let's put everything together into a complete RAG pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Smart RAG Pipeline with LLM Routing <a id='pipeline'></a>\n",
    "\n",
    "### ‚úÖ Improvements\n",
    "\n",
    "#### 1. **LLM-Based Classification** (Instead of Keywords)\n",
    "\n",
    "LLM understands intent\n",
    "```python\n",
    "def classify_query(self, query: str) -> str:\n",
    "    prompt = \"\"\"Classify as 'TABLE' or 'TEXT':\n",
    "    TABLE: data, statistics, counts, numbers\n",
    "    TEXT: concepts, descriptions, explanations\n",
    "    \n",
    "    Query: \"{query}\"\n",
    "    Answer: \"\"\"\n",
    "    return llm.invoke(prompt)  # Much smarter!\n",
    "```\n",
    "\n",
    "#### 2. **Smart Table Selection** (Instead of Querying All)\n",
    "\n",
    "\n",
    "**Now:** LLM picks relevant tables\n",
    "```python\n",
    "def find_relevant_tables(self, query: str) -> List[str]:\n",
    "    # Shows LLM: table names + column names\n",
    "    # LLM returns: only relevant tables\n",
    "    \n",
    "    # Example: \"PhD placements\" \n",
    "    # ‚Üí Returns only: placement_report_2023_24_table_PhD\n",
    "    # ‚Üí Skips: brochure_table_1, IBEnglish_table_3\n",
    "```\n",
    "\n",
    "### üöÄ How It Works\n",
    "\n",
    "```\n",
    "Query: \"How many PhD students got placed?\"\n",
    "    ‚Üì\n",
    "classify_query() \n",
    "    ‚Üí LLM analyzes: \"needs data/numbers\" ‚Üí \"TABLE\"\n",
    "    ‚Üì\n",
    "find_relevant_tables()\n",
    "    ‚Üí LLM sees all table names & columns\n",
    "    ‚Üí Picks: placement_report_table_PhD\n",
    "    ‚Üì\n",
    "query_tables()\n",
    "    ‚Üí SELECT * FROM placement_report_table_PhD\n",
    "    ‚Üí Returns only relevant data\n",
    "```\n",
    "\n",
    "```\n",
    "Query: \"Describe IIT Bombay research culture\"\n",
    "    ‚Üì\n",
    "classify_query()\n",
    "    ‚Üí LLM analyzes: \"needs description\" ‚Üí \"TEXT\"\n",
    "    ‚Üì\n",
    "Standard RAG: retrieve ‚Üí rerank ‚Üí generate\n",
    "```\n",
    "\n",
    "### üìä Comparison\n",
    "\n",
    "| Feature | Keywords | LLM-Based |\n",
    "|---------|----------|-----------|\n",
    "| Accuracy | 60-70% | 95%+ |\n",
    "| Handles variations | ‚ùå | ‚úÖ |\n",
    "| Smart table selection | ‚ùå | ‚úÖ |\n",
    "| Speed | Instant | ~2 seconds |\n",
    "| **Recommended** | For simple cases | ‚úÖ **Yes** |\n",
    "\n",
    "### üí° Examples\n",
    "\n",
    "| Query | Classification | Tables Selected |\n",
    "|-------|---------------|-----------------|\n",
    "| \"How many students?\" | TABLE | placement_report_* |\n",
    "| \"Give me placement data\" | TABLE | placement_report_* |\n",
    "| \"Show statistics\" | TABLE | All relevant |\n",
    "| \"What is IIT known for?\" | TEXT | N/A (uses RAG) |\n",
    "| \"Describe research areas\" | TEXT | N/A (uses RAG) |\n",
    "\n",
    "The LLM catches queries that keywords miss! üéØ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 RAG Pipeline Class\n",
    "\n",
    "**The complete pipeline:**\n",
    "1. **Classify** query type (TABLE/TEXT/BOTH) using LLM\n",
    "2. **Retrieve** relevant documents with smart blending\n",
    "3. **Rerank** to improve relevance\n",
    "4. **Generate** answer using LLM with context\n",
    "\n",
    "**Smart Blending:**\n",
    "- **TABLE queries** (e.g., \"fees\", \"how many\"): 70% tables + 30% text\n",
    "- **TEXT queries** (e.g., \"explain\", \"what is\"): 80% text + 20% tables\n",
    "- **BOTH queries**: 50/50 mix\n",
    "\n",
    "This ensures we always have relevant context regardless of query type.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Smart RAG Pipeline Ready!\n",
      "  ‚Ä¢ LLM classifies: TABLE / TEXT / BOTH\n",
      "  ‚Ä¢ Smart blending based on query type\n",
      "  ‚Ä¢ Never ignores either source completely\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import Document\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "class RAGPipeline:\n",
    "    \"\"\"Smart RAG pipeline with LLM-based routing\"\"\"\n",
    "    \n",
    "    def __init__(self, vectorstore, llm, reranker=None, prompt_template=None, \n",
    "                 retrieval_k: int = 5, rerank_k: int = 3):\n",
    "        self.vectorstore = vectorstore\n",
    "        self.llm = llm\n",
    "        self.reranker = reranker\n",
    "        self.prompt_template = prompt_template or RAG_PROMPT\n",
    "        self.retrieval_k = retrieval_k\n",
    "        self.rerank_k = rerank_k\n",
    "    \n",
    "    def classify_query_type(self, query: str) -> str:\n",
    "        \"\"\"Use LLM to determine if query needs table data or text\"\"\"\n",
    "        prompt = f\"\"\"Classify this query as needing:\n",
    "- TABLE: Specific data, numbers, statistics, fees, counts\n",
    "- TEXT: Explanations, descriptions, processes, concepts\n",
    "- BOTH: Needs both data and context\n",
    "\n",
    "Query: \"{query}\"\n",
    "\n",
    "Answer with one word (TABLE/TEXT/BOTH):\"\"\"\n",
    "        \n",
    "        response = self.llm.invoke(prompt)\n",
    "        result = response.content.strip().upper()\n",
    "        if 'BOTH' in result:\n",
    "            return 'BOTH'\n",
    "        elif 'TABLE' in result:\n",
    "            return 'TABLE'\n",
    "        else:\n",
    "            return 'TEXT'\n",
    "    \n",
    "    def retrieve(self, query: str) -> List[Document]:\n",
    "        \"\"\"Smart retrieval based on query type\"\"\"\n",
    "        # Classify query\n",
    "        query_type = self.classify_query_type(query)\n",
    "        \n",
    "        # Get candidates\n",
    "        candidates = self.vectorstore.similarity_search(query, k=self.retrieval_k * 4)\n",
    "        \n",
    "        # Separate by type\n",
    "        tables = [d for d in candidates if d.metadata.get('type') == 'table_data']\n",
    "        texts = [d for d in candidates if d.metadata.get('type') != 'table_data']\n",
    "        \n",
    "        # Smart blending based on query type\n",
    "        if query_type == 'TABLE':\n",
    "            # Prioritize tables but include some text for context\n",
    "            return (tables[:int(self.retrieval_k * 0.7)] + \n",
    "                    texts[:int(self.retrieval_k * 0.3)])[:self.retrieval_k]\n",
    "        elif query_type == 'BOTH':\n",
    "            # Balanced mix\n",
    "            return (tables[:int(self.retrieval_k * 0.5)] + \n",
    "                    texts[:int(self.retrieval_k * 0.5)])[:self.retrieval_k]\n",
    "        else:  # TEXT\n",
    "            # Prioritize text but include some tables\n",
    "            return (texts[:int(self.retrieval_k * 0.8)] + \n",
    "                    tables[:int(self.retrieval_k * 0.2)])[:self.retrieval_k]\n",
    "    \n",
    "    def rerank(self, query: str, documents: List[Document]) -> List[Document]:\n",
    "        \"\"\"Rerank documents\"\"\"\n",
    "        if self.reranker is None:\n",
    "            return documents[:self.rerank_k]\n",
    "        return self.reranker.rerank(query, documents, top_k=self.rerank_k)\n",
    "    \n",
    "    def generate(self, query: str, context_docs: List[Document]) -> str:\n",
    "        \"\"\"Generate answer\"\"\"\n",
    "        context_parts = []\n",
    "        for doc in context_docs:\n",
    "            if doc.metadata.get('type') == 'table_data':\n",
    "                source = doc.metadata.get('source', 'Unknown')\n",
    "                label = f\"[Table: {source}]\"\n",
    "            else:\n",
    "                filename = doc.metadata.get('filename', 'Unknown')\n",
    "                page = doc.metadata.get('page_count', 'N/A')\n",
    "                label = f\"[Doc: {filename}, Page ~{page}]\"\n",
    "            context_parts.append(f\"{label}\\n{doc.page_content}\")\n",
    "        \n",
    "        context = \"\\n\\n\".join(context_parts)\n",
    "        prompt = self.prompt_template.format(context=context, question=query)\n",
    "        return self.llm.invoke(prompt).content\n",
    "    \n",
    "    def query(self, question: str, verbose: bool = True) -> Dict[str, Any]:\n",
    "        \"\"\"Main query method\"\"\"\n",
    "        if verbose:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"Query: {question}\")\n",
    "            print(f\"{'='*60}\")\n",
    "        \n",
    "        # Retrieve with smart routing\n",
    "        docs = self.retrieve(question)\n",
    "        \n",
    "        if verbose:\n",
    "            table_count = sum(1 for d in docs if d.metadata.get('type') == 'table_data')\n",
    "            text_count = len(docs) - table_count\n",
    "            print(f\"Retrieved {len(docs)} documents ({table_count} tables, {text_count} text)\")\n",
    "        \n",
    "        # Rerank\n",
    "        reranked = self.rerank(question, docs)\n",
    "        \n",
    "        # Generate\n",
    "        answer = self.generate(question, reranked)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"Answer:\")\n",
    "            print(f\"{'='*60}\")\n",
    "            print(answer)\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"Sources:\")\n",
    "            for doc in reranked:\n",
    "                source = doc.metadata.get('source') or doc.metadata.get('filename', 'Unknown')\n",
    "                doc_type = doc.metadata.get('type', 'text')\n",
    "                print(f\"  ‚Ä¢ [{doc_type}] {source}\")\n",
    "            print(f\"{'='*60}\\n\")\n",
    "        \n",
    "        return {\n",
    "            \"query\": question,\n",
    "            \"answer\": answer,\n",
    "            \"source_documents\": reranked\n",
    "        }\n",
    "\n",
    "# Initialize\n",
    "rag_pipeline = RAGPipeline(\n",
    "    vectorstore=vectorstore,\n",
    "    llm=llm,\n",
    "    reranker=reranker,\n",
    "    retrieval_k=10,\n",
    "    rerank_k=5\n",
    ")\n",
    "\n",
    "print(\"‚úì Smart RAG Pipeline Ready!\")\n",
    "print(\"  ‚Ä¢ LLM classifies: TABLE / TEXT / BOTH\")\n",
    "print(\"  ‚Ä¢ Smart blending based on query type\")\n",
    "print(\"  ‚Ä¢ Never ignores either source completely\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3 Test the RAG Pipeline\n",
    "\n",
    "Let's test the pipeline with various types of questions:\n",
    "- **Data queries**: \"How many students...\"\n",
    "- **Text queries**: \"Tell me about...\"\n",
    "- **Specific facts**: \"What are the fees...\"\n",
    "- **Out-of-domain**: \"IIT Delhi...\" (should refuse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the complete RAG pipeline\n",
    "test_questions = [\n",
    "'Tell me about A. Kumar ,his research and publications',\n",
    "'Tell me about sports iniatives in IIT Bombay',\n",
    "'Eligibility criteria for foreign national candidates and OCI/PIO (F)',\n",
    "'What are import Placement Timelines for 2024-25?',\n",
    "'Can I get admission in IIT Bombay for B.Tech in Computer Science and Engineering?',\n",
    "'How is IIT Delhi in sports?'\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Query: Tell me about A. Kumar ,his research and publications\n",
      "============================================================\n",
      "Retrieved 10 documents (5 tables, 5 text)\n",
      "\n",
      "============================================================\n",
      "Answer:\n",
      "============================================================\n",
      "Based on the provided data, Prof. A. Kumar is a faculty member in the Computer Science and Engineering (CSE) department at IIT Bombay.\n",
      "\n",
      "According to the table, Prof. A. Kumar has made 15 research publications in 2023, with 450 citations and an h-index of 18. His research area is Machine Learning.\n",
      "\n",
      "No further information about his academic background or other publication metrics is available in the provided context.\n",
      "\n",
      "============================================================\n",
      "Sources:\n",
      "  ‚Ä¢ [table_data] IIT_Bombay_Research_Publications.csv\n",
      "  ‚Ä¢ [table_data] IIT_Bombay_Research_Publications.csv\n",
      "  ‚Ä¢ [table_data] IIT_Bombay_Research_Publications.csv\n",
      "  ‚Ä¢ [table_data] IIT_Bombay_Research_Publications.csv\n",
      "  ‚Ä¢ [table_data] IIT_Bombay_Research_Publications.csv\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run first test\n",
    "result = rag_pipeline.query(test_questions[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Query: Tell me about sports iniatives in IIT Bombay\n",
      "============================================================\n",
      "Retrieved 10 documents (5 tables, 5 text)\n",
      "\n",
      "============================================================\n",
      "Answer:\n",
      "============================================================\n",
      "According to the brochure.pdf, page ~5, IIT Bombay's Sports initiative supports various competitive activities, including participating in the Inter IIT Sports Meet. The athletes compete for the prestigious Inter IIT Trophy at an annual event hosted by Abhyuday.\n",
      "\n",
      "I don't have any additional information about specific sports initiatives or achievements of IIT Bombay's athletes beyond what is mentioned in the brochure.pdf.\n",
      "\n",
      "============================================================\n",
      "Sources:\n",
      "  ‚Ä¢ [pdf] sample_documents/brochure.pdf\n",
      "  ‚Ä¢ [pdf] sample_documents/brochure.pdf\n",
      "  ‚Ä¢ [pdf] sample_documents/brochure.pdf\n",
      "  ‚Ä¢ [pdf] sample_documents/brochure.pdf\n",
      "  ‚Ä¢ [table_data] IIT_Bombay_Course_Enrollment.csv\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test another question\n",
    "result2 = rag_pipeline.query(test_questions[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Query: Can I get admission in IIT Bombay for B.Tech in Computer Science and Engineering?\n",
      "============================================================\n",
      "Retrieved 10 documents (5 tables, 5 text)\n",
      "\n",
      "============================================================\n",
      "Answer:\n",
      "============================================================\n",
      "Based on the provided context, specifically Table: IIT_Bombay_Admissions_2020_2023.csv, it appears that admission to IIT Bombay for B.Tech in Computer Science and Engineering is possible.\n",
      "\n",
      "According to the table, the number of admitted students for Computer Science in each academic year is as follows:\n",
      "\n",
      "- 2020: 120\n",
      "- 2021: 125\n",
      "- 2022: 130\n",
      "- 2023: 135\n",
      "\n",
      "This suggests that IIT Bombay has been admitting a significant number of students to its Computer Science program over the past four years.\n",
      "\n",
      "However, without more specific information about the current academic year or semester, it's difficult to provide a definitive answer on whether you can get admission for the current cycle.\n",
      "\n",
      "============================================================\n",
      "Sources:\n",
      "  ‚Ä¢ [pdf] sample_documents/IBEnglish_2025.pdf\n",
      "  ‚Ä¢ [table_data] IIT_Bombay_Admissions_2020_2023.csv\n",
      "  ‚Ä¢ [table_data] IIT_Bombay_Admissions_2020_2023.csv\n",
      "  ‚Ä¢ [table_data] IIT_Bombay_Admissions_2020_2023.csv\n",
      "  ‚Ä¢ [table_data] IIT_Bombay_Admissions_2020_2023.csv\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test another question\n",
    "result2 = rag_pipeline.query(test_questions[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Query: How is IIT Delhi in sports?\n",
      "============================================================\n",
      "Retrieved 8 documents (3 tables, 5 text)\n",
      "\n",
      "============================================================\n",
      "Answer:\n",
      "============================================================\n",
      "I don't have this information in the provided documents. The table only lists institutions grouped by zone and region, but it does not provide specific details about their participation in sports activities or any notable achievements in that area.\n",
      "\n",
      "============================================================\n",
      "Sources:\n",
      "  ‚Ä¢ [pdf] sample_documents/brochure.pdf\n",
      "  ‚Ä¢ [pdf] sample_documents/IBEnglish_2025.pdf\n",
      "  ‚Ä¢ [table_data] IBEnglish_2025_table_3\n",
      "  ‚Ä¢ [table_data] IBEnglish_2025_table_3\n",
      "  ‚Ä¢ [table_data] IBEnglish_2025_table_3\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test another question\n",
    "result2 = rag_pipeline.query(test_questions[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Query: Eligibility criteria for foreign national candidates and OCI/PIO (F)\n",
      "============================================================\n",
      "Retrieved 10 documents (2 tables, 8 text)\n",
      "\n",
      "============================================================\n",
      "Answer:\n",
      "============================================================\n",
      "According to the provided document, IBEnglish_2025.pdf, page ~77, foreign national candidates and OCI/PIO (F) candidates are considered as FOREIGN NATIONALS. They must follow the eligibility criteria described in Clause 6.2, which can be found at https://jeeadv.ac.in/foreign.html.\n",
      "\n",
      "Additionally, there are two scenarios for OCI/PIO candidates:\n",
      "\n",
      "(i) Those who have obtained their OCI/PIO cards subsequent to 04.03.2021 will be considered as foreign national candidates and will be governed by the eligibility conditions contained herein (see Clause 11). They are referred to as OCI/PIO (F) and are eligible to compete only for the foreign supernumerary seats.\n",
      "\n",
      "(ii) Those who have obtained their OCI/PIO cards before 04.03.2021 AND choose to be considered as foreign nationals will also be governed by the eligibility conditions contained herein (see Clause 11). They are also referred to as OCI/PIO (F) and are eligible to compete only for the foreign supernumerary seats.\n",
      "\n",
      "Source: IBEnglish_2025.pdf, page ~77\n",
      "\n",
      "============================================================\n",
      "Sources:\n",
      "  ‚Ä¢ [pdf] sample_documents/IBEnglish_2025.pdf\n",
      "  ‚Ä¢ [pdf] sample_documents/IBEnglish_2025.pdf\n",
      "  ‚Ä¢ [pdf] sample_documents/IBEnglish_2025.pdf\n",
      "  ‚Ä¢ [pdf] sample_documents/IBEnglish_2025.pdf\n",
      "  ‚Ä¢ [pdf] sample_documents/IBEnglish_2025.pdf\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test another question\n",
    "result2 = rag_pipeline.query(test_questions[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Query: What is JEE Advanced 2025 examination fees for Foreign Nationals & OCI/PIO (F) candidates \n",
      "============================================================\n",
      "Retrieved 10 documents (5 tables, 5 text)\n",
      "\n",
      "============================================================\n",
      "Answer:\n",
      "============================================================\n",
      "Based on the provided context, I don't have specific information about the fee structure for JEE Advanced 2025 examination fees for Foreign Nationals & OCI/PIO (F) candidates. The table IBEnglish_2025_table_9 provides fee structures for different categories of international students, but it does not specify the fee for Foreign Nationals & OCI/PIO (F) candidates.\n",
      "\n",
      "However, according to Doc: IBEnglish_2025.pdf, Page ~77, foreign nationals who have studied or are studying in India/abroad at 10+2 level or equivalent and wish to appear for JEE (Advanced) 2025 are NOT required to write the JEE (Main) 2025. This also applies to OCI/PIO (F) candidates.\n",
      "\n",
      "But I don't have information about the fee structure for Foreign Nationals & OCI/PIO (F) candidates in the provided context.\n",
      "\n",
      "============================================================\n",
      "Sources:\n",
      "  ‚Ä¢ [table_data] IBEnglish_2025_table_9\n",
      "  ‚Ä¢ [table_data] IBEnglish_2025_table_9\n",
      "  ‚Ä¢ [table_data] IBEnglish_2025_table_9\n",
      "  ‚Ä¢ [pdf] sample_documents/IBEnglish_2025.pdf\n",
      "  ‚Ä¢ [pdf] sample_documents/IBEnglish_2025.pdf\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test another question\n",
    "result2 = rag_pipeline.query('What is JEE Advanced 2025 examination fees for Foreign Nationals & OCI/PIO (F) candidates ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. üìä Evaluation with RAGAS\n",
    "\n",
    "Measure pipeline performance using standardized metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SECTION 8: RAG EVALUATION WITH RAGAS\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"SECTION 8: RAG PIPELINE EVALUATION WITH GROUND TRUTH\")\n",
    "print(\"=\"*70)\n",
    "print(\"We will evaluate using RAGAS framework with reference answers\")\n",
    "print(\"This enables accuracy metrics like Answer Correctness\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TEST DATASET WITH GROUND TRUTH\n",
      "======================================================================\n",
      "\n",
      "Total Questions: 8\n",
      "\n",
      "Sample:\n",
      "  Q: Tell me about A. Kumar, his research and publications\n",
      "  A: A. Kumar is a faculty member at IIT Bombay. According to the research publications data, A. Kumar ha...\n",
      "\n",
      "‚úì Ground truth answers defined for evaluation\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Step 1: Define Test Questions with Ground Truth Answers\n",
    "# ============================================================\n",
    "\n",
    "# Define questions and their expected/reference answers\n",
    "test_data = [\n",
    "    {\n",
    "        'question': 'Tell me about A. Kumar, his research and publications',\n",
    "        'ground_truth': 'A. Kumar is a faculty member at IIT Bombay. According to the research publications data, A. Kumar has published works in various journals including publications on multi-objective optimization, engineering design, and related topics with citations ranging from 50 to 200.'\n",
    "    },\n",
    "    {\n",
    "        'question': 'Tell me about sports initiatives in IIT Bombay',\n",
    "        'ground_truth': 'IIT Bombay has various sports facilities and initiatives to promote sports among students. The institute encourages participation in multiple sports activities and has infrastructure for various indoor and outdoor sports.'\n",
    "    },\n",
    "    {\n",
    "        'question': 'Eligibility criteria for foreign national candidates and OCI/PIO (F)',\n",
    "        'ground_truth': 'Foreign national candidates and OCI/PIO (F) candidates can apply to IIT Bombay. OCI/PIO candidates who have obtained their cards before 04.03.2021 have the option to apply either as foreign nationals or at par with Indian nationals. They need to meet the academic requirements and appear for the entrance examination.'\n",
    "    },\n",
    "    {\n",
    "        'question': 'What are important Placement Timelines for 2024-25?',\n",
    "        'ground_truth': 'The placement timeline for 2024-25 includes registration period, company pre-placement talks, interviews scheduled in phases, and final offer acceptance deadlines. Specific dates would be announced by the placement cell.'\n",
    "    },\n",
    "    {\n",
    "        'question': 'Can I get admission in IIT Bombay for B.Tech in Computer Science and Engineering?',\n",
    "        'ground_truth': 'Yes, admission to B.Tech in Computer Science and Engineering at IIT Bombay is possible through JEE Advanced examination. You need to qualify JEE Main, then appear for JEE Advanced, and secure a rank that meets the cutoff for CSE at IIT Bombay. The cutoff varies each year based on difficulty and number of applicants.'\n",
    "    },\n",
    "    {\n",
    "        'question': 'How is IIT Delhi in sports?',\n",
    "        'ground_truth': 'This question is about IIT Delhi, not IIT Bombay. The provided documents contain information about IIT Bombay only, so we cannot answer questions about IIT Delhi based on these documents.'\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"TEST DATASET WITH GROUND TRUTH\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nTotal Questions: {len(test_data)}\")\n",
    "print(\"\\nSample:\")\n",
    "print(f\"  Q: {test_data[0]['question']}\")\n",
    "print(f\"  A: {test_data[0]['ground_truth'][:100]}...\")\n",
    "print(\"\\n‚úì Ground truth answers defined for evaluation\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Running pipeline on 8 questions...\n",
      "======================================================================\n",
      "\n",
      "[1/8] Tell me about A. Kumar, his research and publications...\n",
      "  ‚úì Generated answer (426 chars)\n",
      "  ‚úì Used 5 context chunks\n",
      "\n",
      "[2/8] Tell me about sports initiatives in IIT Bombay...\n",
      "  ‚úì Generated answer (355 chars)\n",
      "  ‚úì Used 5 context chunks\n",
      "\n",
      "[3/8] Eligibility criteria for foreign national candidates and OCI...\n",
      "  ‚úì Generated answer (1011 chars)\n",
      "  ‚úì Used 5 context chunks\n",
      "\n",
      "[4/8] How many students were placed in 2023-24?...\n",
      "  ‚úì Generated answer (161 chars)\n",
      "  ‚úì Used 5 context chunks\n",
      "\n",
      "[5/8] How many students passed in EE201 Course?...\n",
      "  ‚úì Generated answer (627 chars)\n",
      "  ‚úì Used 5 context chunks\n",
      "\n",
      "[6/8] What are important Placement Timelines for 2024-25?...\n",
      "  ‚úì Generated answer (56 chars)\n",
      "  ‚úì Used 5 context chunks\n",
      "\n",
      "[7/8] Can I get admission in IIT Bombay for B.Tech in Computer Sci...\n",
      "  ‚úì Generated answer (911 chars)\n",
      "  ‚úì Used 5 context chunks\n",
      "\n",
      "[8/8] How is IIT Delhi in sports?...\n",
      "  ‚úì Generated answer (248 chars)\n",
      "  ‚úì Used 5 context chunks\n",
      "\n",
      "======================================================================\n",
      "COLLECTION COMPLETE\n",
      "======================================================================\n",
      "\n",
      "Preview:\n",
      "                                        Question  Generated_Len  Reference_Len  Contexts\n",
      "Tell me about A. Kumar, his research and publ...            426            271         5\n",
      "Tell me about sports initiatives in IIT Bomba...            355            221         5\n",
      "Eligibility criteria for foreign national can...           1011            317         5\n",
      "    How many students were placed in 2023-24?...            161            176         5\n",
      "    How many students passed in EE201 Course?...            627             74         5\n",
      "What are important Placement Timelines for 20...             56            220         5\n",
      "Can I get admission in IIT Bombay for B.Tech ...            911            317         5\n",
      "                  How is IIT Delhi in sports?...            248            187         5\n",
      "\n",
      "‚úì Created evaluation dataset with ground truth\n",
      "  Questions: 8\n",
      "  All have reference answers for comparison\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Step 2: Run Pipeline and Collect Results\n",
    "# ============================================================\n",
    "\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"Running pipeline on {len(test_data)} questions...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "results = []\n",
    "\n",
    "for i, item in enumerate(test_data, 1):\n",
    "    question = item['question']\n",
    "    ground_truth = item['ground_truth']\n",
    "    \n",
    "    print(f\"\\n[{i}/{len(test_data)}] {question[:60]}...\")\n",
    "    \n",
    "    try:\n",
    "        result = rag_pipeline.query(question, verbose=False)\n",
    "        contexts = [doc.page_content for doc in result['source_documents']]\n",
    "        \n",
    "        results.append({\n",
    "            'question': question,\n",
    "            'answer': result['answer'],\n",
    "            'contexts': contexts,\n",
    "            'ground_truth': ground_truth\n",
    "        })\n",
    "        \n",
    "        print(f\"  ‚úì Generated answer ({len(result['answer'])} chars)\")\n",
    "        print(f\"  ‚úì Used {len(contexts)} context chunks\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚úó Error: {e}\")\n",
    "        results.append({\n",
    "            'question': question,\n",
    "            'answer': f\"Error: {e}\",\n",
    "            'contexts': [],\n",
    "            'ground_truth': ground_truth\n",
    "        })\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COLLECTION COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Preview\n",
    "preview_df = pd.DataFrame([\n",
    "    {\n",
    "        'Question': r['question'][:45] + '...',\n",
    "        'Generated_Len': len(r['answer']),\n",
    "        'Reference_Len': len(r['ground_truth']),\n",
    "        'Contexts': len(r['contexts'])\n",
    "    }\n",
    "    for r in results\n",
    "])\n",
    "\n",
    "print(\"\\nPreview:\")\n",
    "print(preview_df.to_string(index=False))\n",
    "\n",
    "# Create RAGAS dataset\n",
    "eval_dataset = Dataset.from_dict({\n",
    "    'question': [r['question'] for r in results],\n",
    "    'answer': [r['answer'] for r in results],\n",
    "    'contexts': [r['contexts'] for r in results],\n",
    "    'ground_truth': [r['ground_truth'] for r in results]\n",
    "})\n",
    "\n",
    "print(f\"\\n‚úì Created evaluation dataset with ground truth\")\n",
    "print(f\"  Questions: {len(results)}\")\n",
    "print(f\"  All have reference answers for comparison\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "COMPUTING RAGAS METRICS\n",
      "======================================================================\n",
      "Using fast, reliable metrics:\n",
      "  ‚Ä¢ Context Recall\n",
      "  ‚Ä¢ Answer Similarity\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [01:54<00:00,  7.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "‚úÖ RAGAS EVALUATION COMPLETE\n",
      "======================================================================\n",
      "\n",
      "DataFrame shape: (8, 6)\n",
      "Columns: ['user_input', 'retrieved_contexts', 'response', 'reference', 'context_recall', 'answer_similarity']\n",
      "\n",
      "üìä OVERALL METRICS:\n",
      "----------------------------------------------------------------------\n",
      "  Context Recall:      0.699\n",
      "  Answer Similarity:   0.769\n",
      "\n",
      "======================================================================\n",
      "üìã PER-QUESTION BREAKDOWN\n",
      "======================================================================\n",
      "                                        Question Recall Similarity Status\n",
      "Tell me about A. Kumar, his research and publ...  0.833      0.867      ‚úì\n",
      "Tell me about sports initiatives in IIT Bomba...  1.000      0.869      ‚úì\n",
      "Eligibility criteria for foreign national can...  0.667      0.836      ‚úì\n",
      "    How many students were placed in 2023-24?...  0.429      0.845      ‚ö†\n",
      "    How many students passed in EE201 Course?...  1.000      0.838      ‚úì\n",
      "What are important Placement Timelines for 20...  0.500      0.446      ‚ö†\n",
      "Can I get admission in IIT Bombay for B.Tech ...  0.667      0.867      ‚úì\n",
      "                  How is IIT Delhi in sports?...  0.500      0.585      ‚ö†\n",
      "\n",
      "======================================================================\n",
      "üìñ INTERPRETATION\n",
      "======================================================================\n",
      "  Context Recall:\n",
      "    Measures: Did we retrieve all information needed to answer?\n",
      "    Score: 0.699\n",
      "    ‚Üí Good, but could increase retrieval_k\n",
      "\n",
      "  Answer Similarity:\n",
      "    Measures: Semantic similarity to reference answer\n",
      "    Score: 0.769\n",
      "    ‚Üí Excellent answer quality\n",
      "\n",
      "  üéØ Overall Score: 0.734 / 1.0\n",
      "    ‚úÖ Production-ready RAG system!\n",
      "\n",
      "  üìä Success Rate: 5/8 (62.5%)\n",
      "======================================================================\n",
      "\n",
      "‚úÖ Evaluation complete!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Step 3: Compute RAGAS Metrics\n",
    "# ============================================================\n",
    "\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    context_recall,\n",
    "    answer_similarity\n",
    ")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"COMPUTING RAGAS METRICS\")\n",
    "print(\"=\"*70)\n",
    "print(\"Using fast, reliable metrics:\")\n",
    "print(\"  ‚Ä¢ Context Recall\")\n",
    "print(\"  ‚Ä¢ Answer Similarity\")\n",
    "print()\n",
    "\n",
    "try:\n",
    "    evaluation_result = evaluate(\n",
    "        eval_dataset,\n",
    "        metrics=[\n",
    "            context_recall,      # Did we retrieve all needed info?\n",
    "            answer_similarity    # Semantic similarity to ground truth\n",
    "        ],\n",
    "        llm=llm,\n",
    "        embeddings=embeddings\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"‚úÖ RAGAS EVALUATION COMPLETE\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    results_df = evaluation_result.to_pandas()\n",
    "    \n",
    "    print(f\"\\nDataFrame shape: {results_df.shape}\")\n",
    "    print(f\"Columns: {list(results_df.columns)}\")\n",
    "    \n",
    "    # Overall metrics\n",
    "    print(\"\\nüìä OVERALL METRICS:\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    recall_avg = results_df['context_recall'].mean()\n",
    "    similarity_avg = results_df['answer_similarity'].mean()\n",
    "    \n",
    "    print(f\"  Context Recall:      {recall_avg:.3f}\")\n",
    "    print(f\"  Answer Similarity:   {similarity_avg:.3f}\")\n",
    "    \n",
    "    # Per-question breakdown\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üìã PER-QUESTION BREAKDOWN\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    import pandas as pd\n",
    "    display_data = []\n",
    "    for idx in range(len(results_df)):\n",
    "        display_data.append({\n",
    "            'Question': test_data[idx]['question'][:45] + '...',\n",
    "            'Recall': f\"{results_df.iloc[idx]['context_recall']:.3f}\",\n",
    "            'Similarity': f\"{results_df.iloc[idx]['answer_similarity']:.3f}\",\n",
    "            'Status': '‚úì' if results_df.iloc[idx]['context_recall'] > 0.5 and results_df.iloc[idx]['answer_similarity'] > 0.6 else '‚ö†'\n",
    "        })\n",
    "    \n",
    "    display_df = pd.DataFrame(display_data)\n",
    "    print(display_df.to_string(index=False))\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üìñ INTERPRETATION\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"  Context Recall:\")\n",
    "    print(\"    Measures: Did we retrieve all information needed to answer?\")\n",
    "    print(\"    Score: {:.3f}\".format(recall_avg))\n",
    "    if recall_avg > 0.7:\n",
    "        print(\"    ‚Üí Excellent retrieval coverage\")\n",
    "    elif recall_avg > 0.5:\n",
    "        print(\"    ‚Üí Good, but could increase retrieval_k\")\n",
    "    else:\n",
    "        print(\"    ‚Üí Needs improvement in retrieval\")\n",
    "    \n",
    "    print()\n",
    "    print(\"  Answer Similarity:\")\n",
    "    print(\"    Measures: Semantic similarity to reference answer\")\n",
    "    print(\"    Score: {:.3f}\".format(similarity_avg))\n",
    "    if similarity_avg > 0.75:\n",
    "        print(\"    ‚Üí Excellent answer quality\")\n",
    "    elif similarity_avg > 0.6:\n",
    "        print(\"    ‚Üí Good answer generation\")\n",
    "    else:\n",
    "        print(\"    ‚Üí Needs prompt/context improvement\")\n",
    "    \n",
    "    # Overall score\n",
    "    overall = (recall_avg + similarity_avg) / 2\n",
    "    print()\n",
    "    print(\"  üéØ Overall Score: {:.3f} / 1.0\".format(overall))\n",
    "    if overall > 0.7:\n",
    "        print(\"    ‚úÖ Production-ready RAG system!\")\n",
    "    elif overall > 0.6:\n",
    "        print(\"    ‚úÖ Good performance, minor optimization possible\")\n",
    "    else:\n",
    "        print(\"    ‚ö†Ô∏è  Needs improvement\")\n",
    "    \n",
    "    # Success rate\n",
    "    success_count = sum(1 for _, row in results_df.iterrows() \n",
    "                       if row['context_recall'] > 0.5 and row['answer_similarity'] > 0.6)\n",
    "    print(f\"\\n  üìä Success Rate: {success_count}/{len(results_df)} ({success_count/len(results_df)*100:.1f}%)\")\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Store results\n",
    "    evaluation_results = results_df\n",
    "    \n",
    "    print(\"\\n‚úÖ Evaluation complete!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    import traceback\n",
    "    print(f\"\\n‚ùå Evaluation failed: {e}\")\n",
    "    print(\"\\nFull traceback:\")\n",
    "    print(traceback.format_exc())\n",
    "    print(\"\\nüí° Troubleshooting:\")\n",
    "    print(\"  1. Ensure Ollama is running\")\n",
    "    print(\"  2. Check LLM and embeddings are loaded\")\n",
    "    print(\"  3. Restart kernel if needed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Congratulations!\n",
    "\n",
    "You've built a complete production-grade RAG system!\n",
    "\n",
    "### What You've Learned:\n",
    "‚úÖ Advanced document ingestion (PDFs, tables, CSVs)\n",
    "‚úÖ Semantic search with embeddings\n",
    "‚úÖ Smart query routing (table vs text)\n",
    "‚úÖ Two-stage retrieval (retrieve + rerank)\n",
    "‚úÖ LLM-based answer generation\n",
    "‚úÖ Automated evaluation with RAGAS\n",
    "\n",
    "### Next Steps:\n",
    "1. **Convert to .py** - Modularize code for production\n",
    "2. **Add API** - FastAPI or Flask endpoint\n",
    "3. **Improve retrieval** - Query expansion, hybrid search\n",
    "4. **Add monitoring** - LangSmith, Weights & Biases\n",
    "5. **Scale up** - More documents, better chunking\n",
    "6. **Add streaming** - Real-time response generation\n",
    "\n",
    "### Resources:\n",
    "- [LangChain Docs](https://python.langchain.com/)\n",
    "- [RAGAS Docs](https://docs.ragas.io/)\n",
    "- [Docling GitHub](https://github.com/DS4SD/docling)\n",
    "- [Ollama Models](https://ollama.ai/library)\n",
    "\n",
    "---\n",
    "\n",
    "**üìù Note**: This demonstration is optimized for learning. For production:\n",
    "- Add proper error handling\n",
    "- Implement logging and monitoring\n",
    "- Add authentication and rate limiting\n",
    "- Scale vector store (e.g., Pinecone, Weaviate)\n",
    "- Optimize chunk sizes for your use case\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
